\chapter{MXNet}

参考文献：\href{https://mxnet.incubator.apache.org/architecture/index.html}{MXNet Architecture}

{\color{red}Time: 2018.05.18}

\section{MXNet System Overview}

今天把这个Overview补上。

\subsection{MXNet System Architecture}

MXNet系统主要的Modules包含：

\begin{itemize}
\item Runtime Dependency Engines: 根据程序的读写依赖对它们进行调度和执行
\item Storage Allocator: 用于高效的分配和回收CPU和GPU上的存储空间
\item Resource Manager: 管理Global资源，比如随机数发生器等
\item NDArray: 动态的、异步的n维array，可以为MXNet提供方便的命令式编程
\item Symbolic Execution: 静态的符号图执行器，可以提供搞笑的符号计算图执行和优化
\item Operator: 定义了前向和反向传播计算的计算子
\item SimpleOp: 以同意的风格扩展NDarray计算子和符号计算子 
\item Symbol Construction: 提供一种定义计算图的方法
\item KVStore: Key-Value 存储接口，用于高效的参数同步
\item Data Loading (IO): 高效的分布式数据加载和增广工具 
\end{itemize}

\subsection{MXNet System Components}

\subsubsection{Execution Engine}

我们不仅可以使用mxnet的engine用于深度学习，而且还可以用于其他领域特定的问题。它主要用于执行一些具有依赖关系的functions。任何有依赖关系的两个函数只能顺序执行，而那些没有依赖关系的计算可以并行执行。

\begin{verbatim}
    using Fn = std::function<void(RunContext)>
\end{verbatim}

所以，函数的类型是\verb|void(RunContext)|。另外一个需要注意的地方是，这里的RunContext与Context的区别，前者包含了只有在运行时才能确定的一些信息，比如函数执行的stream；后者包含了device type与device 看文档，ImageFolderDataset的说明文档。其中

另外一个变量是\verb|VarHandle|，这个主要用于指定function的依赖。在\verb|const_vars|用于存储那些只读变量，而\verb|mutate_vars|用于指定那些可以修改的变量。

\subsubsection{Operators in MXNet}

在MXNet中，每一个操作都是一个类，包含了实际运算逻辑和一些辅助信息，这些辅助信息可以帮助确定可以优化的地方。建议让自己熟悉\verb|mshadow|库，因为所有的计算都是\verb|mshadow::TBlob|这一tensor-like的结构进行的。MXNet里面的运算接口可以允许：

\begin{itemize}    
    \item 指定inplace更新来降低memory allocation cost
    \item 隐藏Python中的一些临时变量，让其更干净
    \item 定义输入与输出张量的关系，可以让系统帮助shape检查
    \item 索取更多的临时变量用于计算，比如cudnn等
\end{itemize}

\subsubsection{Operator Interface}

Forward是核心算子接口，此外还有Backward接口。

\lstset{language=C++}
\begin{lstlisting}
virtual void Backward(const OpContext &ctx,
                          const std::vector<TBlob> &out_grad,
                          const std::vector<TBlob> &in_data,
                          const std::vector<TBlob> &out_data,
                          const std::vector<OpReqType> &req,
                          const std::vector<TBlob> &in_grad,
                          const std::vector<TBlob> &aux_states);
\end{lstlisting}

在Backward中，输入数据时out\_grad, in\_data, 和out\_data，同时计算in\_grad作为函数的结果。有些操作可能不是同时需要上述三个参数，此时可以通过\verb|OperatorProperty|里面的\verb|DeclareBackwardDependency|接口来设置实际的依赖关系。

\subsubsection{Operator Property}

上面说完了两个最重要的核心Operator Interface，在这里，就开始说一下Operator Property了。

单独设置Operator  Property的目的：有时候卷积操作可能有多种不同的实现方式，并且想要通过切换它们来实现最高的性能。因此，我们把Operator 的semantic interfaces和Implementation interface (\verb|Operator|类)分开，并把前者在\verb|OperatorProperty|类中。\verb|OperatorProperty|主要由以下几个部分构成：
\begin{itemize}
\item \verb|InferShape|

这个函数有两个目的：

\begin{itemize}
\item 告诉系统输入、输出tensor的size,因此系统可以在实际调用Forward和Backward之前就可以分配空间了；
\item 在实际运行前，先进行shape的检查，以避免一些明显的错误。同时如果没有足够的信息用于推断出shape时会返回false，如果shape不一致，那么就会throws an error
\end{itemize}

\item \verb|RequestResources|

有一些操作如\verb|cudnnConvolutionForward|在计算过程中需要额外的一些空间。MXNet定义了两个函数分别用于Foward和Backward过程的资源请求，资源请求的结果会保存在ctx中，然后传入Operator中的Forward和Backward函数。具体做法如下：
\begin{lstlisting}
auto tmp_space_res = ctx.requested[kTempSpace].get_space(some_shape, some_stream);
auto rand_res = ctx.requested[kRandom].get_random(some_stream);
\end{lstlisting}

\item \verb|Backward dependency|

用于声明在反向传播过程中所依赖的变量，这样的话，如果发现不需要的变量，那么就可以及早把资源释放掉。

\item \verb|In palce Option|

为了进一步节省存储的消耗，可以使用in-place update。 这个策略适合 element-wise类的操作，并且输入、输出有相同的shape。通过\verb|ForwardInplaceOption|和\verb|BackwardInplaceOption|来实现，这两个函数的返回值说明是可以inplace的两个变量。

需要注意的地方是，及时你用到了这些设置，但也不能保证结果是确实是inplace的。实际上，这只是对系统的一个建议，而不是强制性的，系统还会考虑一些其它因素，但这些因素对开发者是不可见的，因此对于Forward和Backward的实现可以不用考虑这些隐含的策略。

\item \verb|Expose Operator to Python|

由于C++的限制，必须实现以下接口：

\begin{lstlisting}
// initial the property class from a list of key-value string pairs
           virtual void Init(const vector<pair<string, string>> &kwargs) = 0;
           // return the parameters in a key-value string map
           virtual map<string, string> GetParams() const = 0;
           // return the name of arguments (for generating signature in python)
           virtual vector<string> ListArguments() const;
           // return the name of output values
           virtual vector<string> ListOutputs() const;
           // return the name of auxiliary states
           virtual vector<string> ListAuxiliaryStates() const;
           // return the number of output values
           virtual int NumOutputs() const;
           // return the number of visible outputs
           virtual int NumVisibleOutputs() const;

\end{lstlisting}

\end{itemize}

\subsubsection{Create an Operator from the Operator Property}

Operator Property不仅提供一下Semantic 属性，而且还负责创建Operator指针，用于实际计算。

待续...


\section{Optimizing Memory Consumption in DL}
Over the last ten years, a constant trend in deep learning is towards deeper and larger networks. Despite rapid advances in hardware performance, cutting-edge deep learning models continue to push the limits of GPU RAM. So even today, it’s always desirable to find ways to train larger models while consuming less memory. Doing so enables us to train faster, using larger batch sizes, and consequently achieving a higher GPU utilization rate.

\subsection{Computation Graph}
A computation graph describes the (data flow) dependencies between the operations in the deep network. The operations performed in the graph can be either fine-grained or coarse-grained.

The concept of a computation graph is explicitly encoded in packages like Theano and CGT. In other libraries, computation graphs appear implicitly as network configuration files. The major difference in these libraries comes down to how they calculate gradients. There are mainly two ways: performing back-propagation on the same graph or explicitly representing a backwards path to calculate the required gradients.

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.85\textwidth]{MXNet/back_graph}
\caption{The implicitly \& explicitly back-propagation on Graph}
\end{figure}

Libraries like Caffe, CXXNet, and Torch take the former approach, performing back-prop on the original graph. Libraries like Theano and CGT take the latter approach, explicitly representing the backward path. In this discussion, we adopt the explicit backward path approach because it has several advantages for optimization.

We adopt the explicit backward path approach because it has several advantages for optimization.

Why is explicit backward path better? Two reasons:
\begin{itemize}
\item The explicit backward path clearly describes the dependency between computations.\\
Like the following case, where we want to get the gradient of \textbf{A} and \textbf{B}. As we can see clearly from the graph, the computation of the $d(C)$ gradient doesn’t depend on $F$. This means that we can free the memory of $F$ right after the forward computation is done. Similarly, the memory of $F$ can be recycled.
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.85\textwidth]{MXNet/back_dep_prune}
\caption{Dependencies can be found quickly.}
\end{figure}

\item Another advantage of the explicit backward path is the ability to have a different backward path, instead of a mirror of forward one.\\
A common example is the split connectioncase, as shown in the following figure.
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.85\textwidth]{MXNet/back_agg_grad}
\caption{Different backward path from forward path.}
\end{figure}
In this example, the output of \textbf{B} is referenced by two operations. If we want to do the gradient calculation in the same network, we need to introduce an explicit split layer. This means we need to do the split for the forward pass, too. In this figure, the forward pass doesn’t contain a split layer, but the graph will automatically insert a gradient aggregation node before passing the gradient back to \textbf{B}. This helps us to save the memory cost of allocating the output of the split layer, and the operation cost of replicating the data in the forward pass.
\end{itemize}

\subsection{What Can be Optimized?}
As you can see, the computation graph is a useful way to discuss memory allocation optimization techniques. Already, we’ve shown how you can save some memory by using the explicit backward graph. Now let’s explore further optimizations, and see how we might determine reasonable baselines for benchmarking.

Assume that we want to build a neural network with \textit{n} layers. Typically, when implementing a neural network, we need to allocate node space for both the output of each layer and the gradient values used during back-propagation. This means we need roughly $2n$ memory cells. We face the same requirement when using the explicit backward graph approach because the number of nodes in a backward pass is roughly the same as in a forward pass.

\subsubsection{In-place Operations}
One of the simplest techniques we can employ is \textit{In-place memory sharing} across operations. For neural networks, we can usually apply this technique for the operations corresponding to activation functions. 

''In-place'' means using same memory for input and output. But you should be careful about that the result is used by more than one operation! 

\subsubsection{Standard Memory Sharing}
In-place operations are not the only places where we can share memory. In the following example, because the value of B is no longer needed after we compute E, we can reuse B’s memory to hold the result of E.
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.85\textwidth]{MXNet/alloc_normal}
\caption{Standard Memory sharing between \textbf{B} \& the result of \textbf{E}.}
\end{figure}

Memory sharing doesn't necessarily require the same data shape. Note that in the preceding example, the shapes of \textbf{B} and \textbf{E} can differ. To handle such a situation, we can allocate a memory region of size equal to the maximum of that required by \textbf{B} and \textbf{E} and share it between them.

\subsection{Memory Allocation Algorithm}
Based on the '' In-Place Operatioins'', how can we allocate memory correctly?

The key problem is that we need to place resources so that they don’t conflict with each other. More specifically, each variable has a \textbf{life time} between the time it gets computed until the last time it is used. In the case of the multi-layer perceptron, the life time of \textit{fc1} ends after \textit{act1} get computed.
\index{Life Time}
See below figure:
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.85\textwidth]{MXNet/alloc_mlp}
\caption{Standard Memory sharing between \textbf{B} \& the result of \textbf{E}.}
\end{figure}

The principle is to allow memory sharing only between variables whose lifetimes don’t overlap. There are multiple ways to do this. You can construct the conflicting graph with each variable as a node and link the edge between variables with overlapping lifespans, and then run a graph-coloring algorithm. This likely has $O(n^2)$ complexity, where \textit{n} is the number of nodes in the graph. This might be too costly.

Let’s consider another simple heuristic. The idea is to simulate the procedure of traversing the graph, and keep a count of future operations that depends on the node.
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.95\textwidth]{MXNet/alloc_step}
\caption{Standard Memory sharing between \textbf{B} \& the result of \textbf{E}.}
\end{figure}
\begin{itemize}
\item An in-place optimization can be performed when only the current operation depends on the source (i.e. count == 1).
\item Memory can be recycled into the box on the upper right corner when the \textit{count} goes to 0.
\item When we need new memory, we can either get it from the box or allocate a new one. 
\end{itemize}

\textbf{Noet:} During the simulation, no memory is allocated. Instead, we keep a record of how much memory each node needs, and allocate the maximum of the shared parts in the final memory plan.

\subsection{Static vs. Dynamic Allocation}
The major difference is that static allocation is only done once, so we can afford to use more complicated algorithms. For example, we can search for memory sizes that are similar to the required memory block. The Allocation can also be made graph aware. We’ll talk about that in the next section. Dynamic allocation puts more pressure on fast memory allocation and garbage collection.

There is also one takeaway for users who want to rely on dynamic memory allocations: do not unnecessarily reference objects. For example, if we organize all of the nodes in a list and store then in a Net object, these nodes will never get dereferenced, and we gain no space. Unfortunately, this is a common way to organize code.

\subsection{Memory Allocation for Parallel Operations}
In the previous section, we discussed how we can simulate running the procedure for a computation graph to get a static allocation plan. However, optimizing for parallel computation presents other challenges because resource sharing and parallelization are on the two ends of a balance. Let’s look at the following two allocation plans for the same graph:
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.95\textwidth]{MXNet/parallel_alloc}
\caption{Standard Memory sharing between \textbf{B} \& the result of \textbf{E}.}
\end{figure}

Both allocation plans are valid if we run the computation serially, {\bfseries from $A[1]$ to $A[8]$. } However, the allocation plan on the left introduces additional dependencies, which means we can't run computation of $A[2]$ and $A[5]$ in parallel. The plan on the right can. To parallelize computation, we need to take greater care. 

\subsubsection{Be Correct and Safe First}
Being correct is our first principle. This means to execute in a way that takes implicit dependency memory sharing into consideration. You can do this by adding the implicit dependency edge to the execution graph. Or, even simpler, if the execution engine is mutation aware, as described in \href{https://mxnet.incubator.apache.org/architecture/note_engine.html}{our discussion of dependency engine design}, push the operation in sequence and write to the same variable tag that represents the same memory region.

Always produce a safe memory allocation plan. This means never allocate the same memory to nodes that can be parallelized. This might not be ideal when memory reduction is more desirable, and we don’t gain too much when we can get benefit from multiple computing streams simultaneously executing on the same GPU.

\subsubsection{Try to Allow More Parallelization}
Now we can safely perform some optimizations. The general idea is to try and encourage memory sharing between nodes that can’t be parallelized. You can do this by creating an ancestor relationship graph and querying it during allocation, which costs approximately $O(n^2)$ in time to construct. We can also use a heuristic here, 
for example, color the path in the graph. As shown in the following figure, when you try to find the longest paths in the graph, color them the same color and continue.
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.95\textwidth]{MXNet/graph_color}
\caption{Color the longest paths in the Graph.}
\end{figure}

After you get the color of the node, you allow sharing (or encourage sharing) only between nodes of the same color. This is a stricter version of the ancestor relationship, but is costs only $O(n)$ of time if you search for only the first $k$ path.

\subsection{How Much Can we Save ?}
On coarse-grained operation graphs that are already optimized for big operations, you can reduce memory consumption roughly by half. You can reduce memory usage even more if you are optimizing a fine-grained computation network used by symbolic libraries, such as Theano.

\subsection{References}
More details can be found in: \href{https://mxnet.incubator.apache.org/architecture/note_memory.html#how-much-can-you-save}{Opimizing the Memory Consumption in DL(MXNet)}.


\section{Deep Learning Programming Style}

Two of the most important high-level design decisions

\begin{itemize}
\item Whether to embrace the symbolic or imperative paradigm for mathematical computation

\item Whether to build networks with bigger or more atomic operations
\end{itemize}

\subsection{Symbolic vs. Imperative Program}

即：符号式编程 vs. 命令式编程

Symbolic programs are a bit different. With symbolic-style programs, we first define a (potentially complex) function abstractly. When defining the function, no actual numerical computation takes place. We define the abstract function in terms of \textbf{placeholder values}(占位符). Then we can compile the function, and evaluate it given real inputs. 

This operation generates a computation graph (also called a symbolic graph) that represents the computation.

Most symbolic-style programs contain, either explicitly or implicitly, a compile step. 

真正的计算只发生在传入数值之时，在这之前，都没有任何计算发生。

The defining characteristic of symbolic programs is their clear separation between building the computation graph and executing it.For neural networks, we typically define the entire model as a single compute graph.

\subsection{Imperative Programs Tend to be More Flexible}

使用命令式编程，那么任何Python语法都可以使用(Nearly anything),但使用符号式编程时，一些Python特性可能无法使用，如迭代。

当使用Python的符号式编程时，实际实在一个Domain-Specific-Language(DSL)定义的空间中进行编程。

Intuitively, you might say that imperative programs are more native than symbolic programs. It’s easier to use native language features. 

\subsection{Symbolic Programs Tend to be More Efficient}

命令式编程与原生Python的相差不大，所以灵活性很高。但符号式编程更有利于速度、存储优化。

Symbolic programs are more restricted. When we call \textit{Compile} on d, we tell the system that only the value of d is needed. The intermediate values of the computation, is then invisible to us.

\begin{itemize}
\item We benefit because the symbolic programs can then safely reuse the memory for in-place computation. 

\item Symbolic programs can also perform another kind of optimization, called operation folding(图\ref{Symbolic1}). In fact, this is one way we hand-craft operations in optimized libraries, such as CXXNet and Caffe. Operation folding improves computation efficiency.

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{MXNet/Symbolic1.png}
\caption{Operation Folding示意图。}
\label{Symbolic1}
\end{figure}

Note, you can’t perform operation folding in imperative programs, because the intermediate values might be referenced in the future. Operation folding is possible in symbolic programs because you get the entire computation graph, and a clear specification of which values will be needed and which are not.
\end{itemize}

\subsection{Case Study: Backprop and AutoDiff}
\lstset{language=Python}
\begin{lstlisting}[title=toy model,frame=shadowbox]
import numpy as np
    a = np.ones(10)
    b = np.ones(10) * 2
    c = b * a
    d = c + 1
    ...
\end{lstlisting}


\subsubsection{基于命令式编程的自动求导}
循环调用求导函数，直至最开始的输入变量。

利用grad闭包(Closure)来隐含的保存后向计算图。

但一个坏处是，必须保存所有中间变脸的Grad闭包。

\subsubsection{基于符号式编程的自动求导}

可以实现的优化力度更大。

\subsubsection{Analysis}

可以实现的优化的程度，依赖于可以允许的操作(Restrictions on what you can do)。使用符号式编程时，必须明确提供这些约束，所以可进行优化也就更多。

对于命令式编程，可以通过其它的一些方式添加明确的约束。比如一种方法是Context Variable。如：
\begin{verbatim}
with context.NoGradient()
    ...
\end{verbatim}
可以关闭梯度的计算。但这样也不能利用In-Place Calculation来对存储空间进行复用。

其实是一种trade-off between restriction and flexibility.

\subsection{Model Checkpoint}

保存和加载模型。保存模型的时候，需要保存两类变量：网络的结构配置、网络的权重系数。

符号式编程有利于配置的检查。而对于命令式编程，需要保存所有的代码，或者利用符号式指令进行能够顶层封装。

\begin{itemize}
\item Parameter Updates

大部分符号是编程，都是基于数据流图(Data Flow Graphs, DFG)实现的。DFG描述的是计算。但这种方式下，参数的更新不是很方便。一些做法是将更新过程转化为基于命令式的方式实现，而梯度的计算是基于计算图的方式计算。

\item There is No Strict Boundary

这两种风格的框架其实没有很明显的区分。如在命令式编程时，可以借助Just-in-Time(JIT) Compiler来实现一些符号式编程里面的全局优化等好处。
 
\end{itemize}


\subsection{Big vs. Small Operations}

\begin{itemize}
\item Big Operations

主要是一些经典的神经网络层在用，如Fully Connected and Batch Normalize.

\item Small Operations

一些数学上的计算，如矩阵乘、Element-wise Addition.

\end{itemize}

CXXNet, Caffe支持Layer一级的计算，而Theano, Minerva支持更精细的计算。

\begin{itemize}
\item Smaller Operations Can Be More Flexible (小运算更灵活)

可实现的东西多，且建立新的Layer比较简单，直接添加部件就行。

\item Big Operations Are More Efficient (大运算更高效)

可能引起计算、存储上的开支。

\item Compilation and Optimization

对于小运算的优化，计算图支持一下两种优化：
\begin{itemize}
\item Memory Allocation Optimization

重用中间结果的存储空间。也可用于大运算。

\item Operator Fusion

Fuse several small operations into big one.

\end{itemize}

这些优化对小操作十分重要。小操作对编译器也增加了负担。

\item Expression Template and Statically Typed Language

借助Expression Template来产生具体的Kernels.\href{https://github.com/dmlc/mshadow/blob/master/guide/exp-template/README.md}{Template Expression}, 其实底层是基于C++ Template实现的。

\end{itemize}


\subsection{Mix The Approaches}

Amdahl's Law:
\begin{quote}
 If you are optimizing a non-performance-critical part of your problem, you won’t get much of a performance gain
\end{quote}

实际考虑编程Style时，需要综合考虑：性能、灵活性、工程复杂度等。实践表明，混合使用多个Style可以得到更好的性能。

\begin{itemize}
\item Symbolic and Imperative Programs

有两种方式可以实现这种混用：
\begin{itemize}
\item Use imperative programs within symbolic programs as callbacks
\item Use symbolic programs as part of imperative programs
\end{itemize}
如在参数更新中的讨论。如果代码中，混合了Symbolic和Imperative，那么结果是Imperative.但更好的选择是，用支持GPU计算、参数更新的符号式编程框架来开发。

\item Small and Big Operations
\item Choose Your Own Approach
\end{itemize}

\section{Dependency Engine for Deep Learning}

\subsection{Problems in Dependency Scheduling}

\begin{itemize}
\item Data Flow Dependency

Data flow dependency describes how the outcome of one computation can be used in other computations. 

\item Memory Recycling

\item Random Number Generation

A pseudo-random number generator (PRNG) is not thread-safe because it might cause some internal state to mutate when generating a new number. Even if the PRNG is thread-safe, it is preferable to serialize number generation, so we can get reproducible random numbers.

\end{itemize}

\subsubsection{Design a Generic Dependency Engine}

目标是建立一个轻量级、普用的依赖引擎。目标如下：
\begin{itemize}
\item 识别有效的操作
\item 可以调度GPU、CPU存储的依赖，以及处理随机发生器的依赖
\item 引擎不应分配资源，而仅处理依赖
\end{itemize}

步骤如下：

\begin{enumerate}
\item At the beginning, the user can allocate the variable tag, and attach it to each of the objects that we want to schedule.

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.75\textwidth]{MXNet/Tag1.png}
\caption{第一步，给变量分配tag}
\label{Tag1}
\end{figure}

\item 然后调用\textit{push}来告知引擎需要运行的函数、参数等，需要区分读取、写入的参数，分别输入。引擎通过识别上一步的tag来识别变量，这样的好处是不涉及tag具体指向什么，所以引擎可以处理包括变量、函数之类的Everything。

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.75\textwidth]{MXNet/Tag2.png}
\caption{把相应的Function Closure push进依赖分析引擎}
\label{Tag2}
\end{figure}
\end{enumerate}

\subsection{Implementing the Generic Dependency Engine}

基本思想
\begin{itemize}
\item Use a queue to track all of the pending dependencies on each variable tag
\item Use a counter on each operation to track how many dependencies are yet to be fulfilled
\item When operations are completed, update the state of the queue and dependency counters to schedule new operations
\end{itemize}

一个例子如图所示：
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.95\textwidth]{MXNet/Tag3.png}
\caption{一个具体的例子}
\label{Tag3.png}
\end{figure}

\subsection{Discussion}

\begin{itemize}
\item Dynamic vs. Static
\item Mutation vs. Immutable
\end{itemize}

\section{Designing Efficient Data Loaders for DL}

几点重要的考虑：
\begin{itemize}
\item Small File Size
\item Parallel (Distributed) packing of data
\item Fast data loading and online augmentation
\item Quick reads from arbitrary parts of the dataset in the distributed setting
\end{itemize}

\subsection{Design Insight}

为了设计好的IO系统，需要解决两类任务：Data Preparation, Data Loading.

\subsubsection{Data Preparation}

Data preparation describes the process of packing data into a desired format for later processing. 

\begin{itemize}
\item Pack the dataset into small numbers of files
\item Do the packing once
\item Process the packing in parallel to save time
\item Be able to access arbitrary parts of the data easily
\end{itemize}

\subsubsection{Data Loading}

The next step to consider is how to load the packed data into RAM. 

\begin{itemize}
\item Read Continuously
\item Reduce the bytes to be loaded，可以借助于数据压缩等
\item Load and train in different threads
\item Save RAM
\end{itemize}

\subsection{Data Format}

需要选择既高效又方便的数据结构。为了实现这个目标，把二进制数据包装成可以分离的结构。具体，MXNet采用DMLC-Core里面的recordIO类型。

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.95\textwidth]{MXNet/recordIO1.png}
\caption{Binary recordIO数据结构}
\label{recordIO1}
\end{figure}

通过连续读，来避免随机读引入的延时。

这种数据结构的一个好处是，每一个record的长度可以改变。从而支持更好的数据压缩。

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.95\textwidth]{MXNet/recordIO2.png}
\caption{Binary recordIO的一个例子}
\label{recordIO2}
\end{figure}
其中，\textit{resize}把输入图像变为256 * 256大小。


\subsubsection{Access Arbitrary Parts of Data}

The packed data can be logically sliced into an arbitrary number of partitions, no matter how many physical packed data files there are.

在recordIO中借助Magic Number来实现上述目的，具体是使用dmlc-core中的\textit{InputSplit}函数来实现。这个函数极大的帮助了并行实现，因为每一个节点只处理一个\textit{Part}.

\subsection{Data Loading and Preprocessing}

\subsubsection{Loading and Preprocessing on the Fly}

In service of efficiency, we also address multi-threading techniques. 

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.95\textwidth]{MXNet/loadingData1.png}
\caption{并行预处理例子}
\label{loadingData1}
\end{figure}

在加载了大量图像数据后，利用多线程工具(OpenMP)进行并行处理。

\subsubsection{Hide IO Cost Using Threadediter}

一种降低IO影响的办法是：数据预取。具体使用dmlc-core提供的\textit{threadediter}来处理IO. The key of threadediter is to start a stand-alone thread that acts as a data provider, while the main thread acts as a data consumer as illustrated below.

The threadediter maintains a buffer of a certain size and automatically fills the buffer when it’s not full. And after the consumer finishes consuming part of the data in the buffer, threadediter will reuse the space to save the next part of data. 

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.95\textwidth]{MXNet/dataLoading2.png}
\caption{数据预取的示意图，借助Buffer来实现}
\label{loadingData2}
\end{figure}


\subsection{MXNet IO Python Interface}

We make the IO object as an iterator in numpy. By achieving that, the user can easily access the data using a for-loop or calling next() function. Defining a data iterator is very similar to defining a symbolic operator in MXNet.

为了创建一个数据迭代器，需要提供五种类型的参数：

\begin{itemize}
\item Dataset Param: 如路径、输入的尺寸等
\item Batch Param: Batch Size
\item Augmentation Param: 确定数据增广的类型，如翻转等
\item Backedn Param:  控制后台线程，来隐藏数据读取延时
\item Auxiliary Param: 帮助Debug的信息等。
\end{itemize}

通常必须确定\textbf{Dataset Param}和\textbf{Batch Param}。MX Data IO也支持模块化，如以下两种：
\begin{itemize}
\item 自己的高效数据预取。allows the user to write a data loader that reads their customized binary format that automatically gets multi-threaded prefetcher support.

\item 数据转换。image random cropping, mirroring, etc. Allows the users to use those tools, or plug in their own customized transformers

\end{itemize}


\section{Except Handling in MXNet}

MXNet在两类情况下可以抛出异常：
\begin{itemize}
\item MXNet main thread. For \textit{e.g.} Infershape and InferType。 在主线程中处理
\item Spawned threads (生成线程？)
\begin{itemize}
\item By dependency engine for operator execution in parallel。会被rethrown到主线程，进行处理
\item By the iterators, during the data loading, text parsing phase \textit{etc.}
\end{itemize}
\end{itemize}

\subsubsection{Exception Handling for Iterators}

CVIter 使用 PrefetcherIter来加载和解析数据。PrefetcherIter会生成一个Producer线程并后台运行，在主线程(Consumer)中使用这些数据。在Producer线程中可能抛出异常，该异常被发送到主线程。


可能引起线程之间的竞争(Race).To avoid this situation, you should try and iterate through your full dataset if you think it can throw exceptions which need to be handled.

\subsubsection{Except Handling for Operators}

For the operator case, the dependency engine spawns a number of threads if it is running in the \textbf{ThreadedEnginePool} or \textbf{ThreadedEnginePerDevice} mode. The final operator is executed in one of the spawned threads.

If an operator throws an exception during execution, this exception is propagated down the dependency chain. Once there is a synchronizing call i.e. \textbf{WaitToRead} for a variable in the dependency chain, the propagated exception is rethrown.


\textbf{注意}：Please avoid waitalls in your code unless you are confident about your code not throwing exception in any scenario. 因为\textbf{mx.nd.waitall}不支持rethrowing异常。

\section{MXNet-Gluon创建模型}

参考文献：\href{zh.gluon.ai}{Gluon}

\subsection{模型构造}

继承\verb|gluon.nn.Block|来实现，必须重载\verb|forward|函数。在\verb|__init__|函数里面定义模型所需的变量等。

\textbf{需要值得注意的地方是}，在基于自定义的模型构建大的模型时，需要调用的是\verb|__init__|函数的参数；而一旦模型定义好了，开始向里面传入数据时，数据的格式需要根据\verb|forward|函数来确定！

\subsubsection{两种稍微不同的实现}
方式一，是直接利用Gluon预定义的层构建类型变量。
\lstset{language=Python, caption={自定义模型，方式一}， label=MXNetModel0}
\begin{lstlisting}
from mxnet import nd
from mxnet.gluon import nn

class MLP(nn.Block):
    # 声明带有模型参数的层，这里我们声明了两个全链接层。
    def __init__(self, **kwargs):
        # 调用 MLP 父类 Block 的构造函数来进行必要的初始化。这样在构造实例时还可以指定
        # 其他函数参数，例如下下一节将介绍的模型参数 params。
        super(MLP, self).__init__(**kwargs)
        # 隐藏层。
        self.hidden = nn.Dense(256, activation='relu')
        # 输出层。
        self.output = nn.Dense(10)
    # 定义模型的前向计算，即如何根据输出计算输出。
    def forward(self, x):
        return self.output(self.hidden(x))
\end{lstlisting}

方式二，即先定义一个Sequential的类型变量Net，然后在向里面增加\verb|gluon.nn.Layer|等具体的层，或者自己定义的层。
\lstset{language=Python, caption={自定义模型，方式二}， label=MXNetModel1}
\begin{lstlisting}
class NestMLP(nn.Block):
    def __init__(self, **kwargs):
        super(NestMLP, self).__init__(**kwargs)
        self.net = nn.Sequential()
        self.net.add(nn.Dense(64, activation='relu'),
                     nn.Dense(32, activation='relu'))
        self.dense = nn.Dense(16, activation='relu')

    def forward(self, x):
        return self.dense(self.net(x))  #调用要加self

net = nn.Sequential()
net.add(NestMLP(), nn.Dense(20), FancyMLP())

net.initialize()
net(x)
\end{lstlisting}

由于FancyMLP和Sequential都是Block的子类，我们可以嵌套调用他们。

实际使用中，貌似还存在第三种构建方式，直接以函数的形式使用， 如VGG:
\lstset{language=Python, caption={自定义模型VGG11，方式三}， label=MXNetLayer2}
\begin{lstlisting}
import sys
sys.path.append('..')
import gluonbook as gb
from mxnet import nd, init, gluon
from mxnet.gluon import nn

def vgg_block(num_convs, num_channels):
    blk = nn.Sequential()
    for _ in range(num_convs):
        blk.add(nn.Conv2D(
            num_channels, kernel_size=3, padding=1, activation='relu'))
    blk.add(nn.MaxPool2D(pool_size=2, strides=2))
    return blk
conv_arch = ((1,64), (1,128), (2,256), (2,512), (2,512))

def vgg(conv_arch):
    net = nn.Sequential()
    # 卷积层部分
    for (num_convs, num_channels) in conv_arch:
        net.add(vgg_block(num_convs, num_channels))
    # 全连接层部分
    net.add(nn.Dense(4096, activation="relu"), nn.Dropout(.5),
            nn.Dense(4096, activation="relu"), nn.Dropout(.5),
            nn.Dense(10))
    return net

net = vgg(conv_arch)
\end{lstlisting}

\subsection{自定义层}

也需要继承\verb|gluon.nn.Block|以及重载\verb|forward|函数。

\lstset{language=Python, caption={自定义层，方式一}， label=MXNetLayer0}
\begin{lstlisting}
class CenteredLayer(nn.Block):
    def __init__(self, **kwargs):
        super(CenteredLayer, self).__init__(**kwargs)

    def forward(self, x):
        return x - x.mean()
\end{lstlisting}

\subsubsection{带参数的自定义层}

我们还可以自定义含模型参数的自定义层。这样，自定义层里的模型参数就可以通过训练学出来了。我们在“模型参数”一节里介绍了Parameter类。其实，在自定义层的时候我们还可以使用Block自带的ParameterDict类型的成员变量params。顾名思义，这是一个由字符串类型的参数名字映射到Parameter类型的模型参数的字典。我们可以通过get函数从ParameterDict创建Parameter，注意是创建，也就是加入！

\lstset{language=Python, caption={带参数的自定义层，方式一}， label=MXNetLayer1}
\begin{lstlisting}
class MyDense(nn.Block):
    def __init__(self, units, in_units, **kwargs):
        super(MyDense, self).__init__(**kwargs)
        self.weight = self.params.get('weight', shape=(in_units, units))
        self.bias = self.params.get('bias', shape=(units,))

    def forward(self, x):
        linear = nd.dot(x, self.weight.data()) + self.bias.data()
        return nd.relu(linear)
\end{lstlisting}

\subsection{实际例子}

\subsubsection{ResNet}

使用的是自定义层方式一中的方式构建残差块。然后定义更高一级的结构(自定义模型方式二中的方法)，最后再在更高一级进行构架整个模型。层次结构如下：

\lstset{language=Python, caption={ResNet using Gluon}}
\begin{lstlisting}
import sys
sys.path.append('..')
import gluonbook as gb
from mxnet import nd, gluon, init
from mxnet.gluon import nn

# 构建底层
class Residual(nn.Block):
    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):
        super(Residual, self).__init__(**kwargs)
        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,
                               strides=strides)
        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,
                                   strides=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm()
        self.bn2 = nn.BatchNorm()

    def forward(self, X):
        Y = nd.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        return nd.relu(Y + X)

# 构建高一层        
def resnet_block(num_channels, num_residuals, first_block=False):
    blk = nn.Sequential()
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))
        else:
            blk.add(Residual(num_channels))
    return blk
    
# 构建整个模型
net.add(resnet_block(64, 2, first_block=True),
        resnet_block(128, 2),
        resnet_block(256, 2),
        resnet_block(512, 2))        
\end{lstlisting}

需要注意的是，即使在\verb|class|里面也可以调用底层的自定义模块，如在\verb|resnet_block|中调用\verb|Residual|，具体调用方式是调用底层类的构造函数，即\verb|__init__|函数，参数要跟这个函数的参数一致。

\subsubsection{DenseNet}

发现与上面的ResNet类似，所以这里省略。

\section{MXNet中的Deconvolution}

具体实现是， \verb|Conv2DTranspose|等。

由于\verb|Conv2DTranspose|这个函数回引起棋盘状 Artifacts，所以在MXNet里面，还有一个上采样卷积的函数：\verb|mxnet.ndarray.UpSampling()|， \textbf{但是}， 听说目前还不好用啊。

参考\href{https://discuss.gluon.ai/t/topic/3041/14}{FCN讨论区-chen0510566}回答，\verb|gluon.nn.Conv2DTranspose|的文档，\verb|Conv2DTranspose|输出的size是这样计算的：
\begin{displaymath}
\begin{gathered}
Out\_height = (height - 1) * strides[0] - 2 * padding[0] + kernel\_size[0] + output\_padding[0] \\
Out\_width = (width - 1) * strides[1] - 2 * padding[1] + kernel\_size[1] + output\_padding[1]
\end{gathered}
\end{displaymath}

$\uparrow$正确性有待验证。

\section{MXNet读取训练数据}

重要，我发现，对于我这样的新手来说，如何处理数据也是个问题，尤其现在，我感觉一头雾水。

参考文章：

[1] \href{https://www.jianshu.com/p/38d55c9dbdb8}{MXNet常见加载数据的方式-简书}

[2] \href{https://mxnet.incubator.apache.org/api/python/io/io.html}{MXNet API}


有必要对MXNet的数据结构进行说明一下！尤其对于axes的概念。在MXNet中，我认为axes可以取四个值，分别对应以下概念：
\begin{itemize}
\item axes = 0, Batches， 比如batchsize=8, 那么axes就对应这8个样本
\item axes = 1, Channels, 比如输入彩色图像，那么axes对应的就是RGB三个通道
\item axes = 2, Width or Height, 还不太确定，与axes=3共同确定每一个Channel的尺寸
\end{itemize}

接下来就是本小节的重点内容，也就是如果处理训练数据，这是开始的一步，但我感觉也是容易被各种教程忽视的一步，如果这步不清楚，那么会影响整个后面的学习与实践过程。

{\bfseries 正如参考文献中所总结的那样，一共可以分为三类。分别如下！}

\begin{itemize}
\item 基于mxnet.io读取数据
\item 使用mxnet.iamge读取数据
\item \textbf{使用gluon接口读取数据}， 这个部分是重点
\end{itemize}

此外，作者还提到了几种支持的数据结构，貌似就这几种么？
\begin{itemize}
\item .rec文件
\item raw image
\item .lst文件，使用mx.image.ImageIter接口读取，这个接口可以读.rec的文件
\end{itemize}

大概流程：传data iter对象给base\_module，把数据对象变为iterator。
下面分别对三种数据读取方式进行示例。

\subsection{使用mx.io读取数据}

\subsubsection{从内存中读取}

调用\verb|mx.io.NDArrayIter|接口，以MXNet官方的train\_mnist.py为例。

\lstset{language=Python, caption={train\_mnist.py例子}}
\begin{lstlisting}
def read_data(label, image):
   """
   download and read data into numpy
   """
   base_url = 'http://yann.lecun.com/exdb/mnist/'
   with gzip.open(download_file(base_url+label, os.path.join('data',label))) as flbl:
       magic, num = struct.unpack(">II", flbl.read(8))
       label = np.fromstring(flbl.read(), dtype=np.int8)
   with gzip.open(download_file(base_url+image, os.path.join('data',image)), 'rb') as fimg:
       magic, num, rows, cols = struct.unpack(">IIII", fimg.read(16))
       image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)
   return (label, image)


def to4d(img):
   """
   reshape to 4D arrays
   """
   return img.reshape(img.shape[0], 1, 28, 28).astype(np.float32)/255

def get_mnist_iter(args, kv):
   """
   create data iterator with NDArrayIter
   """
   (train_lbl, train_img) = read_data(
           'train-labels-idx1-ubyte.gz', 'train-images-idx3-ubyte.gz')
   (val_lbl, val_img) = read_data(
           't10k-labels-idx1-ubyte.gz', 't10k-images-idx3-ubyte.gz')
   train = mx.io.NDArrayIter(
       to4d(train_img), train_lbl, args.batch_size, shuffle=True)
   val = mx.io.NDArrayIter(
       to4d(val_img), val_lbl, args.batch_size)
   return (train, val)
#参考train_mnist.py
\end{lstlisting}

详解如下：
\begin{itemize}
\item 顶层函数为\verb|get_mnist_iter|函数

该函数的参数是：包含batch size的类arg。kv参数，但貌似没用到。

函数\verb|get_mnist_iter|调用\verb|read_data|函数，返回图像数据及其对应的label。该函数的

将\verb|read_data|函数的返回值传入\verb|mx.io.NDArrayIter|函数，该函数返回名称为\verb|train|和\verb|val|的data iterator. 分别作用与训练图像与训练标签。\verb|NDAarrayIter|函数的使用参数解释在下文单独总结。

返回data iterator的训练图像与训练标签。

\item 被调用函数\verb|read_data|

该函数在这里主要负责下载数据文件(download\_file())、解压缩(struct.unpack)等。各个函数的输入输出可参考源代码。

\item 被调用函数\verb|to4d|

负责把输入图片rehape成四维数据结构，维度分解见上文中的axes的解释。

\end{itemize}

所以总结一下的话，就是这样的，首先下载文件并解压缩，然后读取数据，送入\verb|mx.io.NDArrayIter|产生data iterator.

\subsubsection{从.csv文件中读取数据}

\lstset{language=Python, caption={Read .csv 数据}}
\begin{lstlisting}
#lets save `data` into a csv file first and try reading it back
np.savetxt('data.csv', data, delimiter=',')
data_iter = mx.io.CSVIter(data_csv='data.csv', data_shape=(3,), batch_size=30)
for batch in data_iter:
   print([batch.data, batch.pad])
\end{lstlisting}

详解：
\begin{itemize}
\item 调用\verb|mx.io.CSVIter|读取.csv格式文件，同时输入数据shape以及batch size。返回的就是一个 data iterator。
\end{itemize}

\subsubsection{使用mx.io.ImageRecordIter接口，需要定制KVStore}

这一部分还没看明白，可以看一下原文。

\subsection{常用的系统提供的接口函数}
分析以上实现方式，主要涉及以下几个系统接口函数。
\begin{itemize}
\item \verb|mx.io.NDArrayIter|

原型：class mxnet.io.NDArrayIter(data, label=None, batch\_size=1, shuffle=False, last\_batch\_handle='pad', data\_name='data', label\_name='softmax\_label')

其中，data, label为array 或list of array or dict of string to array。

last\_batch\_handle为最后一个batch的处理，即训练数据的数目不能被batch size整除时的处理方式，可以选取'pad', 'discard' or 'roll\_over'等。

更多细节见文档吧。

Returns an iterator for mx.nd.NDArray, numpy.ndarray, h5py.Dataset mx.nd.sparse.CSRNDArray or scipy.sparse.csr\_matrix.


\item \verb|mx.io.CSVIter|

return the CSV file iterator，详情见文档吧。

\item \verb|struct.unpack|

原型：\verb|struct.unpack(fmt, buffer)|

Unpack from the buffer buffer (presumably packed by pack(fmt, ...)) according to the format string fmt. The result is a tuple even if it contains exactly one item. The buffer’s size in bytes must match the size required by the format, as reflected by calcsize().关于fmt的具体格式，参考\href{https://docs.python.org/3/library/struct.html}{struct library python}

\item \verb|gzip.read()|

原型：\verb|read(size=-1)|

Read and return up to size bytes. If the argument is omitted, None, or negative, data is read and returned until EOF is reached. An empty bytes object is returned if the stream is already at EOF.

If the argument is positive, and the underlying raw stream is not interactive, multiple raw reads may be issued to satisfy the byte count (unless EOF is reached first). But for interactive raw streams, at most one raw read will be issued, and a short result does not imply that EOF is imminent.

所以参数是要读取的byte的数目。

\end{itemize}

\subsection{使用mx.image读取数据}

从imagelist中读取可迭代的数据。

\lstset{language=Python, caption={mx.image使用示例}}
\begin{lstlisting}
data_iter = mx.image.ImageIter(batch_size=4, data_shape=(3, 224, 224), label_width=1,
data_iter.reset()
for data in data_iter:
d = data.data[0]
print(d.shape)
# we can apply lots of augmentations as well
data_iter = mx.image.ImageIter(4, (3, 224, 224), path_imglist='data/custom.lst',
data = data_iter.next()
# specify augmenters manually is also supported
data_iter = mx.image.ImageIter(32, (3, 224, 224), path_rec='data/caltech.rec',
\end{lstlisting}

详解：
\begin{itemize}
\item 调用\verb|mx.image.ImageIter|实现
\end{itemize}

\subsection{使用Gluon接口读取数据}

由于用的比较多，所以这部分是重点，起码目前来看是重点。

使用Gluon接口时，我觉着主要分为两步：产生Dataset，基于Dataset产生 Mini-batch iterator!

下面分别对这两个步骤进行简单说明，后续有时间在详细说一下吧。通过本小节的简单解释，然后再看这个文档，应该就不那么难了吧，就知道各个部分是什么意思了，怎么组织在一起的了。

具体的参考文档：\href{https://mxnet.incubator.apache.org/api/python/gluon/data.html}{Gluon API 文档}

\subsubsection{产生Dataset}

Dataset的生成主要分为几种方式，这里主要讨论三种形式：
\begin{itemize}
\item Gluon自带的数据库，包括：MNIST, FashionMNIST, CIFAR10, CIFAR100等。

产生Dataset的方式为：

\begin{verbatim}
data_set = gluon.data.vision.Dataset
\end{verbatim}

其中， Dataset需要替换成提到的提供的自带数据库。

\item 来自目录生成Dataset， 调用：\verb|ImageFolderDataset|， 目录中保存的raw image。

产生Dataset的方式：

\begin{verbatim}
data_set = gluon.data.vision.ImageFolderDataset(root, flag=1, tranform=None)
\end{verbatim}

A dataset for loading image files stored in a folder structure like:看文档，ImageFolderDataset的说明文档。其中,flag=1说明把读入的图像转换成三通道彩色的，若flag=0那么读入的是灰度图像。

\item 自己生成Dataset类！

产生自定义Dataset的方式：

\begin{verbatim}
class myDatasetName(gluon.data.Dataset)
    ... other functions, to process image as your need
    def __getitem__(..):
        ....
    def __len__(..):
        ....
\end{verbatim}

从上面的代码可以看出，生成自定义的Dataset时，需要继承\verb|gluon.data.Dataset|类，同时，必须override \verb|__getitem__|和\verb|__len__|两个函数！实际使用中，该类可以按照自己数据库的格式，在myDatasetName类中定义其它帮助处理数据的函数，一个更具体点的例子是\href{http://zh.gluon.ai/chapter_computer-vision/fcn.html}{FCN-Gluon}这个文档的代码！

\end{itemize}

\subsubsection{产生Mini-batch iterator}

在上一步，\textbf{无论用哪一种方式}产生了Dataset后，就可以送入\verb|DataLoader|产生data iterator了！

这一步主要通过借助\verb|gluon.data.Dataloader|这一Iterator类来实现。此外还有一些基于Sample的函数，具体看文档。

产生Mini-batch Iterator的方式：

\begin{verbatim}
data_iter = gluon.data.DataLoader(dataset, batch_size=Noen,  
                                          shuffle=False, ....)
\end{verbatim}

后面还有很多参数，可以去文档查看。

这里的第一个参数：\verb|dataset|也就是上文提到的生成Dataset的结果了。

\section{Gluon中的数据结构}

这一部分，我主要想总结一下Gluon中的各种部件的数据结构，如Net = gluon.nn.Sequentional()后，net的数据结构到底是什么样的，还有就是网络的参数又是什么结构的， 比如Parameter类到底长什么样，又该如何使用等。有时间慢慢写吧。

填下坑。

我们可以通过[]来访问Sequential类构造出来的网络的特定层。对于带有模型参数的层，我们可以通过Block类的params属性来得到它包含的所有参数， \textbf{注意这里的Block类是所有neural network lyear和model的基类}！如：\verb|net[0].params|会返回parameter dictionary。

既然\verb|params|会返回parameter dictionary， 那么就可以通过名字来访问字典里面的元素，也可以直接使用它的变量名。下面的两种方法是等价的，但通常后者的代码可读性更好。补充一下这个字典的Key-Value的类型， Key是net的层名字，如dense0\_, conv0\_等； Value是类型是\verb|gluon.nn.Parameter|类型。

\begin{verbatim}
net[0].params['dense0_weight']

net[0].weight
\end{verbatim}

后者貌似不是Block的成员了，通过看Conv2D为例，\verb|weight|是\verb|Conv2D|的一个类成员，其它layer类型应该也有吧,还包括bias等。

上面两种方式，我们都得到了一个\verb|Parameter|类型的变量了。该类型支持\verb|lr_mult|等类型成员了，而且，支持通过\verb|data()|和\verb|grad()|等函数来获取具体的数据。

最后，基类Block提供了\verb|collect_params|函数来获取这个实例包含的所有的参数，他返回的同样是一个Parameter Dictionary， 该字典类型还支持\verb|initialize|和\verb|reset_ctx|等等等等。

\section{MXNet中的Confusing参数}

\subsection{mx.symbol.reshape}

\begin{verbatim}
reshape(data=None, shape=_null, reverse = _Null, target_shape=_Null, target_shape=
            _Null, keep_highest=_Null, name=None, attr=None, out=None, **kwargs)
\end{verbatim}

需要注意的点是，\verb|shape|参数可以取一些特殊的值$\in \{0, -1, -2, -3, -4\}$。它们的含义如下：
\begin{itemize}
\item 0: Copy this dimension from the input to the output shape
\item -1: refers the dimension of the output shape by using the remainder of the input dimensions keeping the size of the new array same as that of the input array. At most one dimension of shape can be -1
\item -2: Copy all/remainder of the input dimensions to the output shape
\item -3: use the product of two consecutive dimensions of the input shape as the output dimension
\item -4: split one dimension of the input into two dimensions passed subsequent to -4 in shape
\end{itemize}

此外，还需要注意一下\verb|reverse|参数的设置问题，具体的更多讲解、例子参考\href{https://mxnet.incubator.apache.org/api/python/symbol/symbol.html?highlight=reshape#mxnet.symbol.reshape}{API文档}。


\subsection{mx.symbol.transpose}

\begin{verbatim}
transpose(data=None, axis=_Null, name=None, attr=None, out=None)
\end{verbatim}

其中\verb|axis|表明输出数据的axis顺序， 默认是取反。

另一个很重要的点是\verb|axis|的具体取值，如$axis=(a, b, c)$， 这句话的具体操作时：输出数据中的第0维，对应的是输入数据中的第$a$维，输出数据中的第1维是对应输入数据中的第$b$维，输出数据中的第2维对应的是输入中的第$c$维。

例如：输入是 高 * 宽 * 通道数，那么要转换成 通道数 * 高 * 宽，那么axis应该取(2, 0, 1)。

\section{UpSample In MXNet}

这一部分主要总结今天看到的所有关于MXNet中UpSample使用的网上帖子。

参考[1]：\href{https://mxnet.incubator.apache.org/api/python/symbol/symbol.html?highlight=upsampling#mxnet.symbol.UpSampling}{UpSampling-MXNet API}

\verb|mxnet.symbol.UpSampling(*data, **kwargs)|

上式用于对输入实现最近邻/双线性插值。其中，\verb|*data|可以选择：data, scale, num\_filter, sample\_type, multi\_input\_mode, workspace, name, 以及\verb|num_args|。

参考[2]: \href{https://github.com/apache/incubator-mxnet/issues/4134}{使用Deconvolution作为UpSampling来使用}

原文题目：How to use mx.sym.UpSampling for bilinear upsampling， 提问者不知道如何设置UpSampling的参数，下面有答案说可以用Deconvolution来实现分辨率的提高，具体的跟DLTips一章里面的用法是一样的，也就是如何设置参数来成倍的提高分辨率。

另一个讨论者通多对\verb|src/operator/upsampling.cc|代码分析可知，当创建Bilinear Upsampling operator时候，其实会返回Deconvolution Opeartioin。 所以可以确定的是，Deconvolution可以用于双线性插值，而要完成这个，只要保证\verb|upsample_weight|被初始化为Bilinear Kernel。 但讨论者有另外一个疑问：就是在后向传播的时候，\verb|upsampling_weight|也会更新，但这可以通过设置网络层的学习率来决定更新或者不更新。

在这个问题下面，有回答者又给出了另一个链接：\href{https://github.com/apache/incubator-mxnet/pull/9688}{BilinearResize2D-PR}， 貌似是5月份刚提交的，所以可能需要更新下MXNet。

参考[3]: \href{https://github.com/apache/incubator-mxnet/issues/1514}{Deconvolution layer的参数配置}

下面piiswrong的回答同样是DLTips那一章节的提升Scale倍分辨率时的参数配置。

参考[4]: \href{https://github.com/apache/incubator-mxnet/issues/1412}{不理解UpSampling是如何工作的}

还是piiswrong的回答：Blinear Upsampling是基于Convolution实现的，因此需要权重，如果不想在训练过程中更新这些参数，需要设置\verb|lr_scale|为0！

提问者的回答：只要不更新权重系数，就可以当做Bilinear Interpolation来用，但如果跟新参数的话，就会被当做普通的Deconvolution来用，piiswrong认为这样更好一点。

那么具体怎么实现呢？

piiswrong的回答是需要一个weight matrix. 可以被mx.init 中的 bilinear initializer进行初始化。

一个例子：
\lstset{language=Python, caption={UpSampling的一个例子}}

\begin{lstlisting}
#upsample filter
def upsample_filt(size):
    factor = (size + 1) // 2
    if size % 2 == 1:
        center = factor - 1
    else:
        center = factor - 0.5
    og = np.ogrid[:size, :size]
    return (1 - abs(og[0] - center) / factor) * \
           (1 - abs(og[1] - center) / factor)

# network
net = mx.symbol.Variable('data')
net = mx.symbol.UpSampling(net, num_filter=3, scale=2, 
			sample_type='bilinear', num_args = 2, name="up")

#upsampling weights
filt = upsample_filt(4)
initw = np.zeros((3,1,4,4), dtype=np.float32)
initw[range(3), 0, :, :] = filt
initw = mx.nd.array(initw)
#forward
c_exec = net.bind(ctx=mx.cpu(), args={'data': im_data, 'up_weight': initw})
c_exec.forward()
\end{lstlisting}

最后，给出一个底层一点的解释：
\begin{quote}
Each upsample layer consists of (1) Upsample (bilinear, zero padding, nearest- neighbor) and (2) convolution with a transposed filter. That is why you need the convolution operation in these layers. Convolution always reduce the input matrix to the output, to go to a bigger output, you have to upsample the input first and potentially do zero - padding to the output later on in order to fix the dimensions. Also, the filters are called learned filters, because the un-transposed version (filter) that we used to calculate the transposed filter learned the features during training.
\end{quote}
在代码里面，需要注意的是，当使用\verb|Bilinear|算法的时候，要设置参数为\verb|num_args=2|。另一个非常重要的是：在权重参数初始化的时候，用的字典的key是\verb|up_weight|!!!

参考[5]: \href{https://github.com/apache/incubator-mxnet/issues/9138}{不能让mxnet.ndarray.UpSampling实现Bilinear插值}

主要有两点值得关注的：
\begin{itemize}
\item From zhanghang1989: \href{https://github.com/apache/incubator-mxnet/pull/9688}{PR} about \verb|BilinearResize2D|和\verb|AdaptiveAvgPooling2d|
\item From \href{https://stackoverflow.com/questions/47897924/implementing-bilinear-interpolation-with-mxnet-ndarray-upsampling}{Stack Overflow Answers}

\verb|mxnet.ndarray.UpSampling| seems to expect 2 inputs (1 input and 1 weight) for bilinear sample\_type.

Also, I think documentation for num\_args parameter is missing which you can check here: \href{https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/upsampling-inl.h#L78}{Github Souce code}

This should work:

\lstset{language=Python, caption={UpSample例子2}}
\begin{lstlisting}
import mxnet as mx
import mxnet.ndarray as nd
xx = nd.random_normal(shape=[1,1,256,256],ctx=mx.cpu())
xx1 = nd.random_normal(shape=[1,1,4,4],ctx=mx.cpu()) 
temp = nd.UpSampling(xx,xx1, num_filter=1, scale=2, sample_type='bilinear', num_args=2)
\end{lstlisting}

\end{itemize}

参考[6]: \href{https://discuss.gluon.ai/t/topic/2823}{nd.UpSampling到底怎么使用Bilinear}

UpSampling的源码可以在Github上找到，里面说：

\begin{quote}
For bilinear upsampling this must be 2; 1 input and 1 weight.
\end{quote}
\textbf{后来改成这种就可以了}：
\begin{lstlisting}
scale = 2
seq.add(nn.Conv2DTranspose(128, kernel_size=(2*scale, 2*scale), 
		strides=(scale, scale), padding=(scale/2, scale/2), 
		weight_initializer=mx.init.Bilinear()))
\end{lstlisting}

参考[7]: \href{https://discuss.gluon.ai/t/topic/2672}{UpSampling-Gluon论坛}

\textbf{重点}。

这个问题下面的Chinakook回答的比较好。

\begin{quote}
严格意义的Upsample应该用Conv2DTranspose，upsampling到指定大小至少到现在我还没看到有任何提升。

upsampler = nn.Conv2DTranspose(channels=channels, 
		kernel\_size=4, strides=2, padding=1, use\_bias=False,
		groups=channels, weight\_initializer= mx.init.Bilinear()) 
		
upsampler.collect\_params().setattr(‘lr\_mult’, 0.)
\end{quote}

\subsection{总结}

回顾上面的几个帖子，总结一下可以用以下三个方法：
\begin{itemize}
\item 使用Conv2DTranspose + mx.init.Bilinear() + net.collect\_params().setattr('lr\_mult', 0)实现, 可以见参考[2, 6, 7]
\item 使用UpSample实现，可以见参考[4, 5]
\item 使用最新的\verb|BilinearResize2D|
\end{itemize}


\section{MXNet中设置层的学习率}

参考：\href{https://discuss.gluon.ai/t/topic/6636}{MXNet如何在命令行设置层的学习率}

Gluon的话可以用\verb|layer.weight.set_lr_mult|来改。

\section{SoftmaxCrossEntropyLoss释疑}

API:

\begin{verbatim}
SoftmaxCrossEntropyLoss(axis=-1, sparse_label=True, from_logits=False, 
									 weight=None, batch_axis=0, **kwargs)
\end{verbatim}

主要说一下\verb|sparse_label|和\verb|axis|两个参数。前者表明输入的label是整数array还是一个概率分布！

如果是一个整数，那么损失的计算方式是：

\begin{displaymath}
L = - \sum_{i}\log p_{i, label_i}
\end{displaymath}

此时输入的label是一个去掉axis哪一维同时保持其它维度的数据尺寸。

而如果输入的是一个概率分布，那么交叉熵的计算方式是：

\begin{displaymath}
L = - \sum_{i} \sum_{j}^{class number} label_j \log p_{ij}
\end{displaymath}

此时输入的label要与输入的pred的尺寸是完全一致的。

也就是说，如果输入是sparse=True， 则每一个label是一个整数标量；而如果输入的是sparse=False， 那么输入的每一个label是一个尺寸是class number的array， 表示该点的概率分布，这里的概率分布是Multinoulli。

再说一下\verb|axis|， 表示在该维度上进行计算。

\section{MXNet中的3D CNN}

\subsection{应该怎么理解3D CNN}

参考：\href{https://www.zhihu.com/question/266352189}{卷积神经网络中的二维卷积核和三维卷积核有什么区别-知乎}

首先需要明确一点的是：在讨论卷积核的维度时，是不把channel维加进来的(或者说卷积核的维度指的是进行划窗操作的维度，而划窗操作是不在channel维度上进行的，因为每个channel共享同一个划窗位置，但每个channel上的卷积核权重是独立的)。所以，二维conv的卷积核其实是(c, h, w)，3D 的卷积核就是(c, d, h, w)，其中多出来的d就是第三维，根据具体应用，在视频中对应的就是时间维，在CT图像中对应就是层数维。

\section{向MXNet添加新的操作}

\subsection{添加新的Operator}

参考[1]: \href{https://blog.csdn.net/u011765306/article/details/54562282}{mxnet学习笔记-自定义Op-CSDN}

需要注意以下几点：


\subsubsection{自定义Op}

自定义Op都需要去继承operator.py中的类，其中提供以下几类：
\begin{itemize}
\item CustomOp(object)
\item CustomOpProp(object)
\item NDArrayOp(PythonOp)
\item NumpyOp(PythonOp)
\item PythonOp(object)
\end{itemize}

这里可以看出，operator分成两条路线，一条路线为CustomOp，另一条路线为继承PythonOp，下面分别介绍下。

\subsubsection{CustomOp类}

这条路线由三步组成。

\begin{itemize}
\item 第一步继承CustomOp,重写方法\verb|forward()|和\verb|backward()|
\item 然后继承\verb|CustomOpProp|重写成员方法，并在方法\verb|create_operator|中调用之前写好的Op
\item 第三步调用\verb|operator.register|对操作进行注册
\end{itemize}

下面就结合具体的代码来说一下：

第一步是继承CustomOp定义自己的Operator类，并重载\verb|forward()|和\verb|backward()|两个函数。
\lstset{language=Python,caption={基于CustomOp的Softmax}}
\begin{lstlisting}
class Softmax(mx.operator.CustomOp):
    # 这里的的in_data就是Prop里面的list_augments返回的数据所包含的内容
    def forward(self, is_train, req, in_data, out_data, aux):  # length of in_data: 2, 也就是list_augments()里面返回数据的length
        x = in_data[0].asnumpy()  # x的shape就是Prop里面的data的shape，
        # 而data的shape就是(batch_size, softmax输入data的shape！)
        # in_data[1]的shape就是Prop里面label的shape。在这里，具体的shape是(batch_size, )，即每一个样本对应一个label!
        y = np.exp(x - x.max(axis=1).reshape((x.shape[0], 1)))     # y 的 shape变成x的shape也就是输入data的shape
        # 下面这一步会涉及Python里面的Broadcast机制！
        y /= y.sum(axis=1).reshape((x.shape[0], 1))  
        self.assign(out_data[0], req[0], mx.nd.array(y))  # 输出的值就是mx.nd.array(y), shape=x.shape
    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):
        l = in_data[1].asnumpy().ravel().astype(np.int)   # in_data[1]对应着labels
        y = out_data[0].asnumpy()                         # 对应forward()的输出，也就是out_augments()的返回值的的第一个元素
        # 知乎上有推导，貌似softmax的求导只需要在响应位置上-1就可，并且其它地方的值是不变的！！
        y[np.arange(l.shape[0])), l] -= 1.0               # l.shape[0]对应着batch_size, l对应每个sample的label的值
        self.assign(in_grad[0], req[0], mx.ndarray(y))    # 返回的是y， 对应的shape还是forward()里面x的shape
\end{lstlisting}

其中，\verb|req|是list of \{ 'null', 'write', 'inplace', 'add'  \}， 确定如何assign to out\_data. null means skip assignment \textit{etc.}  \verb|assign|函数定义在基类CustomOp中，定义如下：

\begin{verbatim}
def assign(self, dst, req, src):
    if req = 'null'  # 什么也不做
        return
    elif req == 'write' or req == 'inpalce':
        dst[:] = src
    elif req == 'add':
        dst[:] += src
\end{verbatim}

第二步是继承mx.operator.CustomOpProp，定义SoftmaxProp类，该类共包含六个函数(包含\verb|__init__|)，代码如下：
\lstset{language=Python, caption={第二步，基于CustomOpProp的SoftmaxProp类}}
\begin{lstlisting}
@mx.operator.register("softmax")
class SoftmaxProp(mx.operator.CustomOpProp):
    def __init__(self):
        super(SoftmaxProp, self).__init__(need_top_grad=False)
    def list_augments(self):  
        return ['data', 'label']
    def list_outputs(self):    
        return ['output']
    def infer_shape(self, in_shape):   # 第一次调用时，会运行这个进行shape的推理
        data_shape = in_shape[0]
        label_shape = in_shape[0][0]
        output_shape = in_shape[0]
        # 返回的的三个[]表示aux_data的shape
        return [data_shape, label_shape], [output_shape], []
    def infer_type(self, in_type):     # 同样也是第一次调用时运行
        return in_type, [in_type[0]], [] 
    def create_operator(self, ctx, shapes, dtypes):
        return Softmax()   # 返回Softmax类
\end{lstlisting}

上述代码就完成了对Softmax的自定义，在类Softmax中重写\verb|forward()|和\verb|backward()|，这里与caffe中定义层操作类似，forward中定义层的前向操作，backward中定义反向传播的梯度计算。在完成定义之后，在类SoftmaxProp中的\verb|create_operator()|调用并返回\verb|Softmax()|实例。那么第三步\verb|register|如何实现呢？可以看到\verb|SoftmaxProp|中带有装饰器\verb|mx.operator.register()|，等价于操作\verb|register("custom_op")(CustomOpProp)|，这里即在代码运行前即完成了该Op的实例化，与Optimazer的装饰器类似，关于这个register的解释见单独介绍。

完成上述操作符的定义后，就可以使用新的操作符了，具体是使用\verb|mx.symbol.Custon()|函数，具体的调用形式如下：
\begin{lstlisting}
data_sym = mx.symbol.Custom(data=fc3, name='softmax', op_type='softmax')
\end{lstlisting}

需要注意的地方在于：1)fc3是softmax上一层网络的输出数据的名字，2)name是本层softmax的名字, 3)是op\_type，这里指定的是自己上面新定义的操作类的名字，或者说是注册时用到的名字。

最后调用Module进行测试：
\begin{lstlisting}
mod = mx.mod.Module(data_sym, context=mx.cpu())   # 等
mod.fit(...)
\end{lstlisting}

其中，data\_sym是symbol模型的最后的输出的变量的名字。

需要注意的地方是，例子程序里面的注释，以及register和两个类的创建过程！！

\subsubsection{PythonOp类}

在这条路线中，PythonOp为基类，而我们大多数定义Op时不会去继承它，而是使用它的subclass：\verb|NDarrayOp|、\verb|NumpyOp|。这条路线不需要像CustomOp那样需要三步。

具体的官网例子可以查看：\href{https://github.com/ROCmSoftwarePlatform/mxnet/blob/master/example/numpy-ops/ndarray_softmax.py}{NDarray Softmax-github}

在这个例子中，类的定义为\verb|class NDarraySoftmax(mx.operator.NDArrayOp)|。类内部主要包含六个函数(一个为init函数)。

纵观整个实现代码，NDarraySoftmax类继承自NDArrayOp，后者其实与NumpyOp类似，不同之处在于\verb|forward()|和\verb|backward()|重写方式使用函数不停，NDArrayOp中使用mx.nd中的操作，但NumpyOp使用numy中的操作。总之重点在\verb|forward()|和\verb|backward()|。当然，如此的自定义方法在使用时需要预先定义类对象才可以使用，基于CustomOp中的定义时机不同。

成员方法\verb|list_augments()|和\verb|list_outputs|和\verb|infer_shape|。第一个函数主要对该Op定义是形参的命名，如代码中的['data', 'label']，那么该Op在使用时形参必须为data和label!这里也可以看出mxnet是使用名字寻找变量的，DataIter和Optimizer也是如此。第二个函数定义了Op的输出变量的名字，一般命名规则是：op\_name + \_'output'。最后一个函数是用于在给定输入时，获取该Op输出的Shape。当然，在自定义Op时，需要自己设计Op的输入和输出的Op。在这个具体的例子中，输出包含两个List，第一个List是输入data和label的Shape，第二个List是输出output的Shape。输入的Shape也就是包含data和label。其中，data的shape就是输入data的shape的第一个维度，也就是样本的个数，而label的shape是输入in\_shape的第二个维度，也就是每个样本的Channel的个数；然后就是输出的shape，这个数值根据Op的操作来确定，这里的具体数值等价于输入in\_shape的第一个维度的取值，也就是样本的个数。

在定义好Op后，我们需要通过\verb|mx.mod.Module()|将Op进行整合，并通过\verb|bind()|来申请内存，在此之后，我们可以通过以下两种方法训练它：
\begin{itemize}
\item 分别调用\verb|init_params()|初始化参数，调用\verb|init_optimizer()|初始化Optimizer，接下来就可以通过\verb|forward()|和\verb|backward()|进行前向、反向传播训练模块。
\item 或者直接调用\verb|fit()|方法进行训练，因为\verb|fit()|中包含所需的初始化操作。
\end{itemize}

\subsection{添加新的Layer}

参考[1]: \href{https://blog.csdn.net/qq_25491201/article/details/51284416}{MXNet创建新的操作(层)-CSDN}

如果需要创建自定义层，有3个选择：

\begin{itemize}
\item 使用Python原生的语言和它的矩阵库(numpy in python)。这不需要过多的能力和对MXNet的了解，但会影响性能
\item 使用mxnet原生的语言和它的矩阵库mxnet.rtc和mxnet.ndarray。这会得到更好的性能，但是相应的需要了解更多的MXNet的知识，可以通过Python来写CUDA的Kernel函数，并且在运行时编译
\item 使用C++/mshadow (CUDA)，这需要对mxnet、mshadow和CUDA都比较熟悉
\end{itemize}

参考里面给出了基于Python/Numpy的一个例子，步骤如下：

\begin{itemize}
\item 定义新的类，继承自mx.operator.NumpyOp
\item 定义函数，返回输入、输出的数据列表，名称分别为：\verb|list_arguments(self)|和\verb|list_outputs(self)|

推荐的返回数据的顺序为：['input1', 'input2', ..., 'weight1', 'weight2',...]

\item 提供\verb|infer_shape|来声明我们的output/weight并且检测输入的形状的一致性

第一个维度总是batch size.\verb|infer_size|需要返回两列，即使他们是空的。

\item 定义\verb|forward()|、|\verb|backward()|函数

forward函数含有两个参数：\verb|in_data|和\verb|out_data|，backward()函数含有四个参数：\verb|out_grad|， \verb|in_data|， \verb|out_data|和 \verb|in_grad|

\end{itemize}

\subsubsection{如何创建新的网络层-CSDN}

参考[2]: \href{https://blog.csdn.net/xuezhisdc/article/details/54927635}{如何创建新的网络层-CSDN}

如果需要自定义一个网络层，比如新的损失函数，有两个选择：
\begin{itemize}
\item 借助前端语言(比如Python),使用CustomOp来写新的操作符。既可以运行在CPU上，也可以运行在GPU上。根据实际情况，性能可能很高，也可能很低。
\item 使用C++/mshadow(CUDA)。但前提是需要熟悉MXNet、mshadow和CUDA，能获得最佳的性能。
\end{itemize}

这里还是以上面的具体的Softmax代码为例子。需要补充说明的一点是，\verb|assign|函数会根据req的取值来决定对out\_data[0]的赋值方式，req取值包括：write, add或null等。

Softmax类定义了新的自定义操作符的计算，但仍然需要通过定义\verb|mx.operator.CustomOpProp|的子类的方式，来定义它的I/O格式！！

在\verb|infer_shape|里面，声明输出和权重的形状，并检查输入形状的一致性。输入/输出张量的第一维是数据的批量大小(Batch size)。标签是一组整数，每个整数对应一个样本，并且输出、输入的形状相同。函数\verb|infer_shape|的输出是三个列别，及时某一项为空，顺序也必须是：输入、输出和辅助状态。

重要的一点：如果想使用自定义操作符，需要创建\verb|mx.sym.Custom|符号。其中，使用新操作符的注册名字来设置参数\verb|op_type|，具体代码如下：
\begin{verbatim}
mlp = mx.symbol.Custom(data=fc3, name='softmax', op_type='softmax')
\end{verbatim}

\subsubsection{MXNet添加新层-CSDN}

参考[3]: \href{https://blog.csdn.net/qq_20965753/article/details/66975622}{MXNet添加新层-CSDN}

具体的是，本文将开始讲一下如何基于C++、CUDA来添加新的操作，而不是上面的那些Python了！具体添加的操作时Softplus函数，然后就可以使用\verb|mx.sym.Softplus|进行调用了。

首先额外说明一下Softplus的计算公式：
\begin{displaymath}
f(x) = \log (1 + exp(x))
\end{displaymath}

具体的步骤如下：

1. 在src/operator下面写好三个文件：-inl.h, .cc, .cu等

2. 对于激活层，在src/operator/mshadow\_op.h加入对\verb|mshadow_op::softplus|、\verb|mshadow_op::softplus_grad|结构体的描述。

\verb|struct softplus {...}|只需要修改return语句即可，y=f(x)，计算y

\verb|struct softplus_grad {...}| 只需要修改return语句即可，计算f的导数f'(x)

具体代码如下：

\lstset{language=C++, caption={在mshadow\_op.h里面添加结构体}}
\begin{lstlisting}
/*! \brief Softplus */
strcut softplus{
	template<typename DType> 
	MSHADOW_XINLINE static DType Map(DType a)
	{
	    return DType(log1pf(a))
	}
};

struct softplus_grad
{
	template<typename DType>
	MSHADOW_XINLINE static DType Map(DType a)
	{
		return DType(DType(1.0f) - exp(-a)
	}
};
\end{lstlisting}

重新编译mxnet即可，将python/mxnet/ lib库文件，重新拷贝到python2.7下的site-packages或Python3的site-packages(或dist-packages)下。在mxnet文件夹外验证：

\begin{verbatim}
>>> import mxnet as mx
>>> help(mx.sym.Softplus)
\end{verbatim}

源码分析。

MXNet输入数据格式是TBlob，输出数据格式是Tesnor，两者都是高维的数组类型。mshadow用于数据存储结构。代码较多，内容待续。



\section{关于MXNet里面的register修饰符}

\subsection{调用顺序-CSDN博客}

在添加自己的softmax的例子中，一个需要值得注意的地方就是register装饰器的理解问题。这里还是参考一个CSDN博客！

参考[1]: \href{https://blog.csdn.net/u011765306/article/details/54562266}{MXNet-Python源码中的装饰器理解-CSDN}

这里主要注意的地方是调用顺序。简单来说，顺序如下：首先执行类方法register，然后执行构建基类，在构建基类的派生类。

稍具体来说：调用类静态方法(@staticmethod)是不需要构建对象的，即调用基类的静态方法时，还不会创建基类的对象，而且在debug里面可以发现register是在调用这个基类的静态函数之前执行的。然后创建基类的对象，最后调用create函数什么的创建派生类。

总而言之一句话：装饰器运行在代码运行之前，即定义阶段就已经完成了。

\subsection{Python中的装饰器之register}














