\chapter{Tips in DL}

重要的一点说明：CNN中的卷积操作其实是信号处理中的相关操作(Correlation Operation)。

\section{Enlarge the FOV}

增加网络的感受野。目前看到的主要方法如下：

\begin{itemize}
\item CRFs\cite{marvin2018crf}
\item Global Graph-reasoning module\cite{chen18iterative}
\item Pooling
\item Dilated conv\cite{YuKoltun2016}
\item 
\end{itemize}


\section{Upsampling}

在卷积以及Pooling之后保持分辨率。目前看到的主要方法如下：

\begin{itemize}
\item Padding
\item Deconvolution
\item Uppooling
\item Bilinear
\item 
\end{itemize}


\section{Multiscale Ability}

在目标检测(Object Detection)中加入多尺度信息。记得的有以下几个方法：

\begin{itemize}
\item Pyramid Network
\item Stacked CNN ?
\item 
\end{itemize}


\section{Dilated Convolution}

主要原理如下。

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.75\textwidth]{DLTips/DilatedConv1}
\caption{Dilated Convolution示意图}
\label{DilatedConv1}
\end{figure}

{\bfseries 注意，下文提到的N-Dilated Conv中的$N={1, 2, 3, \ldots}$是指图中相邻红点之间的间隔。}

图\ref{DilatedConv1}中，(a)图对应3x3的1-dilated conv，和普通的卷积操作一样，(b)图对应3x3的2-dilated conv，实际的卷积kernel size还是3x3，但是空洞为1，也就是对于一个7x7的图像patch，只有9个红色的点和3x3的kernel发生卷积操作，其余的点略过。也可以理解为kernel的size为7x7，但是只有图中的9个点的权重不为0，其余都为0。 可以看到虽然kernel size只有3x3，但是这个卷积的感受野已经增大到了7x7{\bfseries（如果考虑到这个2-dilated conv的前一层是一个1-dilated conv的话，那么每个红点就是1-dilated的卷积输出，所以感受野为3x3，所以1-dilated和2-dilated合起来就能达到7x7的conv）},(c)图是4-dilated conv操作，同理跟在两个1-dilated和2-dilated conv的后面，能达到15x15的感受野。对比传统的conv操作，3层3x3的卷积加起来，stride为1的话，只能达到(kernel-1)*layer+1=7的感受野，也就是和层数layer成线性关系，而dilated conv的感受野是指数级的增长。

Dilated的好处是不做Pooling算是信息的情况下，加大了感受野，让每个卷积核输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的Sequence信息依赖的问题中，都能很好的应用Dilated Convolution, 比如图像分割、语音合成WaveNet、机器翻译ByteNet。

WaveNet的例子。
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.75\textwidth]{DLTips/WaveNet1}
\caption{Dilated Convolution在WaveNet中的应用示意图}
\label{WaveNet1}
\end{figure}

参考文献：\href{https://www.zhihu.com/search?type=content\&q=dilated\%20CNN}{Dilated Conv 知乎}

\section{Deconvolutional Network}

务必看\textbf{补充}部分的内容！

参考文献：\href{https://www.zhihu.com/question/43609045/answer/132235276}{Deconvolution Networks}

可能应用的领域：Visualization, Pixel-wise Prediction, Unsupervised Learning, Image Generation.

大致可分为以下几个方面：
\begin{itemize}
\item Unsupervised Learning 

其实是 Covolutional Sparse Coding. 这里的Deconv只是观念上和传统的Conv反向，传统的conv是从图片生成feature map，而deconv是用unsupervised的方法找到一组kernel和feature map，让它们重建图片。

\item CNN Visualization

通过deconv将CNN中conv得到的feature map还原到像素空间，以观察特定的feature map对哪些pattern的图片敏感，这里的deconv其实不是conv的可逆运算，只是conv的transpose，所以tensorflow里一般取名叫transpose\_conv。

\item Upsampling

在pixel-wise prediction比如image segmentation[4]以及image generation[5]中，由于需要做原始图片尺寸空间的预测，而卷积由于stride往往会降低图片size， 所以往往需要通过upsampling的方法来还原到原始图片尺寸，deconv就充当了一个upsampling的角色。

\end{itemize}

下面主要介绍这三个方面的论文。

\subsection{Convolutional Spare Coding}

{\bfseries 第一篇：Deconvolutional Netwoks}

主要用于学习图片的中低层级的特征表示，属于Unspervised Feature Learning。

更多内容参考本小节的参考文献。

\subsection{CNN可视化}
ZF-Net中利用Deconv来做可视化，它是将CNN学习到的Feature Map的卷积核，取转置，将图片特征从Feature Map空间转化到Pixel空间，用于发现哪些Pixel激活了特定的Feature Map，达到分析理解CNN的目的。


\subsection{Upsampling}
用于FCN\cite{Fcn2014}和DCGAN。

\subsection{补充}

{\bfseries 一个非常好的可以看到多种卷积操作动作图的资源：}\href{https://github.com/vdumoulin/conv_arithmetic}{Convolution Arithmetic Github}


上面的东西还是没有说明白Deconvolution到底是怎么回事啊。

\begin{quote}
实际使用中，反卷积会引起棋盘状的Artifacts。所以要采用上采样卷积层。
\end{quote}
上面这句话是什么意思？

\subsection{Deconvolution与Upsample的区别}

参考文献：\href{https://www.zhihu.com/question/63890195}{Caffe中的Deconvolution和Upsample区别-知乎}

高票回答：

Deconvolution

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{DLTips/Deconvolution0.jpg}
\caption{Transpose Convolution过程示意图}
\label{Deconvolution0}
\end{figure}

Input pixel * filter = output window，不同output window重合的部分使用sum叠加处理。

这一解释和caffe的定义保持一致，caffe中定义解释过来就是：“DeconvolutionLayer 逐像素地将输入值乘上一个filter，并将结果输出windows叠加起来”

Convolve the input with a bank of learned filters, and (optionally) add biases, treating filters and convolution parameters in the opposite sense as ConvolutionLayer.ConvolutionLayer computes each output value by dotting an input window with a filter; DeconvolutionLayer multiplies each input value by a filter elementwise, and sums over the resulting output windows. In other words, DeconvolutionLayer is ConvolutionLayer with the forward and backward passes reversed. DeconvolutionLayer reuses ConvolutionParameter for its parameters, but they take the opposite sense as in ConvolutionLayer (so padding is removed from the output rather than added to the input, and stride results in upsampling rather than downsampling).

Upsampling

该层权重通过BilinearFiller初始化，因此当学习率为0时，权重在训练过程中保持初始值不变，一一直作为bilinear resize的作用。

\lstset{language=Python, caption={Bilinear filter initializer in MXNet}}
\begin{lstlisting}
class Bilinear(Initializer)
"""Initialize weight for upsampling layers."""
def __init__(self):
    super(Bilinear, self).__init__()
def _init_weight(self, _, arr):
    weight = np.zeros(np.prod(arr.shape), dtype='float32')
    shape = arr.shape
    f = np.ceil(shape[3] / 2.)
    c = (2 * f - 1 - f % 2) / (2. * f)
    for i in range(np.prod(shape)):
        x = i % shape[3]
        y = (i / shape[3]) % shape[2]
        weight[i] = (1 - abs(x / f - c)) * (1 - abs(y / f - c))
    arr[:] = weight.reshape(shape)
\end{lstlisting}

James Liu的回答：

\begin{itemize}
\item \textbf{Deconvolution}

上采样就是把$[W,H]$大小的feature map $F_{W,H}$扩大为$[nW,nH]$尺寸大小的$\hat{F}_{nW,nH}$，其中$n$为上采样倍数。那么可以很容易的想到我们可以在扩大的feature map $\hat{F}$上每隔$n$个位置填补原F中对应位置的值。

但是剩余的那些位置怎么办呢？deconv操作是把剩余位置填0，然后这个大feature map过一个conv。

\textbf{所以}：Deconv = 扩大 + 填0 + Convolution

\item \textbf{Upsampling}

插值上采样 = 扩大 + 插值

\end{itemize}

\subsubsection{理解深度学习中的Deconvolution Networks}

参考文献：\href{https://www.zhihu.com/question/43609045}{理解深度学习中的Deconvolution Networks - 知乎}


逆卷积(Deconvolution)比较容易引起误会，转置卷积(Transposed Convolution)是一个更为合适的叫法.

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.5\textwidth]{DLTips/Deconvolution1.png}
\caption{一个例子，用于卷积操作说明}
\label{Deconvolution1}
\end{figure}
输入矩阵可展开为16维向量，记作x

输出矩阵可展开为4维向量，记作y

卷积运算可表示为y = Cx

不难想象C其实就是如下的稀疏阵
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth]{DLTips/Deconvolution2.jpg}
\caption{卷积操作时的矩阵形式}
\label{Deconvolution2}
\end{figure}

那么当反向传播时又会如何呢？首先我们已经有从更深层的网络中得到的$\frac{\partial Loss}{\partial y}$.

\begin{displaymath}
\frac{\partial{Loss}}{\partial{x_j}} = \sum_{i}\frac{\partial{Loss}}{\partial y_i} \frac{\partial y_i}{\partial{x_j}} = \sum_{i} \frac{\partial{Loss}}{\partial y_i} C_{i, j} = \frac{\partial{Loss}}{y} \cdot C_{*, j} = C_{*, j}^T \frac{\partial{Loss}}{\partial y}
\end{displaymath}

回想第一句话，你猜的没错，所谓逆卷积其实就是正向时左乘$C^T$，而反向时左乘$(C^T)^T$，即C的运算。

为什么CS321n里面说要使用Convolution Transpose而不是Deconvolution呢：

反卷积的数学含义，通过反卷积可以将通过卷积的输出信号，完全还原输入信号。

而事实是，转置卷积只能还原shape大小，不能还原value.

\subsection{Deconvolution输出的大小计算}

好吧，我初步知道是怎么做的了：

首先看正向卷积时，假设输入时$1 * 1 * N_{in} * N_{in}$得到的Feature Map是(1 * 1 * $N_{out}$ * $N_{out}$)，该过程中，假设步长为$S$， 卷积核为$K$, Padding为$P$，那么输入输出大小的关系是：
\begin{displaymath}
N_{out} = \frac{N_{in} - K + 2 * P}{S} + 1
\end{displaymath}

那么在转置卷积时，要得到与输入同等大小的Feature Map，需要确定Transpose Convolution的步长、卷积核大小、Padding的大小，那么应该怎么求呢，只需要把上式取反函数就行：
\begin{displaymath}
N_{in} = (N_{out} - 1) * S + K - 2 * P
\end{displaymath}
一般来说，$S$取正向卷积计算中的Strides大小，$K$也可以去相同的，也可以取不同的，这时候只需要确定K与P的合适关系就行。

起码MXNet中的Conv2Transpose可以这样设置参数。

\subsection{Conv2DTranspose用于成倍的提高分辨率的时候}

参考文献：\href{https://discuss.gluon.ai/t/topic/2823/4}{UpSampling中使用Bilinear到底怎么用-Gluon}

一种简单的做法，假设需要提升的分辨率的倍数是$scale$那么设置Transpose Convolution的参数如下：
\begin{verbatim}
gluon.nn.Conv2DTranpose(output_channels, kernel_size=(2*scale, 2*scale) 
							strides=(scale, scale), padding=(scale/2, scale/2), 
							weight_initializer=mx.init.Bilinear()))
\end{verbatim}

比如，要把Feature Map放大2倍，则$scale=2$带入上面的代码就行。


\section{Dilated Network与Deconv Network之间的区别}
Dilated Convolution主要用于增加感受野，而不是Upsampling；Deconv Network主要用于Upsample，即增加图像分辨率。

对于标准的$k \times k$的卷及操作，stride为$s$，分为一下几种情况：
\begin{itemize}
\item $s > 1$

即卷积的同时做了降采样，输入Feature Map的分辨率\footnote{分辨率是指像素的多少，而尺度是指模糊程度的大小，即Gaussian Filter中的方差$\delta$}下降。但这一般也会增加感受野。

\item $s = 1$

普通的步长为1的卷积，输入与输出分辨率相同。

\item $\mathbf{0 < s < 1}$

Fractionally strided convolution.相当于图像做upsampling。比如$s=0.5$时，意味着图像像素之间padding一个空白的像素(像素值为0)后，stride改为1进行卷积，达到一次卷积看到的空间范围变大的目的。

\end{itemize}

\section{目标检测中的mAP的含义}

\begin{itemize}
\item 对于类别C,在一张图像上

首先计算C在一张图像上的精度。

\begin{displaymath}
\label{PrecisionC1}
Precision_C = \frac{N(TP)_C}{N(Total)_C}
\end{displaymath}
其中，$Precision_C$为类别C在一张图像上的精度。$N(TP)_C$为算法检测正确(True Positive)的$C$的个数，检测是否正确按照$IoU > 0.5$算，同理，$T(Total)_C$为这一张图像所有$C$类的个数。所以则一步，仅涉及一个类别$C$以及一张图像。

\item 对于类别C，在多张图像上

这一步计算的是类别$C$的$AP$指数。

\begin{displaymath}
\label{PrecisionC2}
AveragePrecision_C = \frac{\sum Precision_C}{N(TotalImage)_C}
\end{displaymath}
其中，$AveragePrecision_C$是类别$C$的$AP$指数，$Precision_C$为上文计算得到的类别$C$的在一张图像上的精度，然后对所有包含类别$C$的图像上的$C$的精度$Precision_C$求和；$N(TotalImage)_C$为包含类别$C$的图像的数量，也对应于分子中求和所涉及的图像。

\item 在整个数据集上，多个类别

$mAP$在上一步的计算结果的基础上，计算所有类别的$AP$和 / 总的类别数。

\begin{displaymath}
\label{PrecisionC3}
meanAveragePrecision = \frac{\sum_{C} AveragePrecision_C}{N(Class)}
\end{displaymath}
也就是相当于计算所有类别的$\mathbf{AP}$的平均值，是对应于类别总数的平均值。

\end{itemize}

参考文献：\href{https://www.zhihu.com/question/53405779}{知乎文章}

\section{统计学习方法}

一个比较好的总结：\href{http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/#SVM%E3%80%81SMO}{机器学习常见算法个人总结}


\section{Distillation Module}
文献来源：\cite{Xu2018PADNet}\cite{Mehta2018OD200}

在\cite{Xu2018PADNet}中，同时完成深度估计以及场景解析两个任务。

Distillation Module的目的：
\begin{itemize}
\item Deep-model distilation modules fuses information from the intermediate predictions for each specific final task\cite{Xu2018PADNet}.高效的利用中间任务的信息互补。文章\cite{Xu2018PADNet}提出的三种不同的实现方式如图\ref{ThreeDistillationModules1}所示。

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.75\textwidth]{DLTips/ThreeDistillationModules1.png}
\caption{三种不同的Distillation Module}
\label{ThreeDistillationModules1}
\end{figure}

\item Distillation loss function\cite{Mehta2018OD200}

利用Distillation帮助将Teacher Network(精度更高)的知识迁移到的Student Network.

\end{itemize}

\subsection{Knowledge Distillation}

\subsubsection{什么是Distilling the knowledge}

一句话总结，就是用teacher network的输出作为soft label来训练一个student network.

\subsubsection{Disilling the knowledge in a Neural Network}

\begin{displaymath}
q_i = \frac{exp(z_i/T)}{\sum_{j}exp(z_j/T)}
\end{displaymath}

其实就是一个Softmax，值得注意的是$T$是Temperature，$T$越大，概率分布就越Soft。在训练Student Network时，该概率分布就是Student Network的soft label.

Student Network 的训练策略：
\begin{itemize}
\item 先用Teacher Network的概率分布训练
\item 再用Real Label训练
\end{itemize}

\subsection{Recurrent Knowledge Distillation \cite{Silvia2018Recurrent}}

\section{光流估计中的Average end-point error}

貌似就是类似于均方误差类似。具体定义还没查到。

\section{CNN中的卷及方式汇总}

参考文献：\href{https://zhuanlan.zhihu.com/p/29367273}{CNN中的卷积方式2017.9-知乎}

\subsection{Inception}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{DLTips/Inception0.jpg}
\caption{Inception卷积结构}
\label{Inception0}
\end{figure}
如图\ref{Inception0}所示。
主要思想是，融合Network in Network的思想来增加隐层提升非线性表达的思想。先用1*1的卷积映射到隐空间，再在隐空间做卷积操作。同时考虑多尺度，在单层Inception中，用多个大小不同的卷积核做卷积，然后把结果Concat起来。

代表模型：
\begin{itemize}
\item Inception-V1

就是把图\ref{Inception0}中的结构进行Stack,GoogLeNet?

\item Inception-V2

加入了Batch Normalization正则，去除了5*5卷积，用两个3*3卷积代替

\item Inception-V3

7 × 7卷积被拆分成7 * 1 + 1 * 7, 可分离卷积？

\item Inception-V4

加入了残差结构。

\end{itemize}

\subsection{空洞卷积, Dilation}

Dilation卷积，通常译作空洞卷积或者卷积核膨胀操作，它是解决pixel-wise输出模型的一种常用的卷积方式。一种普遍的认识是，pooling下采样操作导致的信息丢失是不可逆的，通常的分类识别模型，只需要预测每一类的概率，所以我们不需要考虑pooling会导致损失图像细节信息的问题，但是做像素级的预测时（譬如语义分割），就要考虑到这个问题了。

所以就要有一种卷积代替pooling的作用（成倍的增加感受野），而空洞卷积就是为了做这个的。通过卷积核插“0”的方式，它可以比普通的卷积获得更大的感受野。

所以，本意是为了增加感受野。

应用：
\begin{itemize}
\item FCN
\item WaveNet
\end{itemize}

\subsection{深度可分离卷积, Depthwise Separable Convolution}

是Incepion的延续。

\begin{figure}[!htbp]
\centering
\subfigure[简化的Inception]{\label{Inception1}
\includegraphics[width=0.95\textwidth]{DLTips/Inception1.jpg}}
\\
\subfigure[Depthwise Separable Convolution的示意图， 可以看出来，与Inception很像。]{\label{DepthwiseSeparable0}
\includegraphics[width=0.95\textwidth]{DLTips/DepthwiseSeparable0.jpg}}
\caption{Inception结构与Depthwise Separable Convolution结构对比}
\label{InceptionAndDepthwise0}
\end{figure}

如图\ref{InceptionAndDepthwise0}所示，我们又可以看做，把一整个输入做1*1卷积，然后切成三段，分别3*3卷积后相连。注意的是，在三个不同的卷积时，是对Channel进行分组进行3*3卷积。

OK，现在我们想，如果不是分成三段，而是分成5段或者更多，那模型的表达能力是不是更强呢？于是我们就切更多段，切到不能再切了，正好是Output channels的数量（极限版本, 图\ref{DepthwiseSeparable1}）：

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{DLTips/DepthwiseSeparable1.jpg}
\caption{Channel分组的极限版本}
\label{DepthwiseSeparable1}
\end{figure}

于是，就有了深度卷积（depthwise convolution），深度卷积是对输入的每一个channel独立的用对应channel的所有卷积核去卷积，假设卷积核的shape是[filter\_height, filter\_width, in\_channels, channel\_multiplier]，那么每个in\_channel会输出channel\_multiplier那么多个通道，最后的feature map就会有in\_channels * channel\_multiplier个通道了。反观普通的卷积，输出的feature map一般就只有channel\_multiplier那么多个通道。

也就是说，对于每一个Channel， 都用不同的多个卷积核进行卷积，具体的是Channel\_multiplier个不同的卷积核。

既然叫深度可分离卷积，光做depthwise convolution肯定是不够的，原文在深度卷积后面又加了pointwise convolution，这个pointwise convolution就是1*1的卷积，可以看做是对那么多分离的通道做了个融合。

这两个过程合起来，就称为Depthwise Separable Convolution了。

应用：
\begin{itemize}
\item Xception
\end{itemize}

\subsection{可变性卷积}

可形变卷积的思想很巧妙：它认为规则形状的卷积核（比如一般用的正方形3*3卷积）可能会限制特征的提取，如果赋予卷积核形变的特性，让网络根据label反传下来的误差自动的调整卷积核的形状，适应网络重点关注的感兴趣的区域，就可以提取更好的特征。

如图\ref{DeformableConv0}：网络会根据原位置（a），学习一个offset偏移量，得到新的卷积核（b）（c）（d），那么一些特殊情况就会成为这个更泛化的模型的特例，例如图（c）表示从不同尺度物体的识别，图（d）表示旋转物体的识别。

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{DLTips/DeformableConv0.jpg}
\caption{Deformable Convolution示意图}
\label{DeformableConv0}
\end{figure}

具体实现如下。

%图\ref{DeformableConv1}中包含两处卷积，第一处是获取offsets的卷积，即我们对input feature map做卷积，得到一个输出（offset field），然后再在这个输出上取对应位置的一组值作为offsets。假设input feature map的shape为[batch，height，width，channels]，我们指定输出通道变成两倍，卷积得到的offset field就是[batch，height，width，2×channels]，为什么指定通道变成两倍呢？因为我们需要在这个offset field里面取一组卷积核的offsets，而一个offset肯定不能一个值就表示的，最少也要用两个值（x方向上的偏移和y方向上的偏移）所以，如果我们的卷积核是3*3，那意味着我们需要3*3个offsets，一共需要2*3*3个值，取完了这些值，就可以顺利使卷积核形变了。第二处就是使用变形的卷积核来卷积，这个比较常规。（这里还有一个用双线性插值的方法获取某一卷积形变后位置的输入的过程）

图\ref{DeformableConv1}中包含两处卷积。第一处是绿色的获取Offsets的卷积，即图中上方部分，这一处的输入是Input feature map，得到一个输出(Offset Feild)， 然后再在这个输出上取一个卷积核大小的切片，作为卷积核对应位置的Offset。若Input Feature的尺寸为：$Batch, height, width, channels$， 其中，Batch表示批数， Channel表示输入的通道数，然后输出通道变为两倍，卷积得到的Offset Field就是$Batch, height, width, 2 *  channels$，那么为什么会翻倍呢， 也就是图中上半部分中的2N是什么意思呢？首先要确定一个卷积核的Offset，需要在不同的方向上单独确定，比如X, Y方向，所以就变成二倍了。在实际进行Deformable时，首先从Offset Field中对应位置上取一个对应卷积核的切片，如果卷积核是3 * 3，那么我们在两个方向都取一个3 * 3的切片，代表来年各个方向的Offset，然后就完成卷积核形变了。

第二处的卷积，是图中下方的表示，就是一个常规的卷积运算，只不过卷积核是经过上述Offset之后的变形卷积核。这里还有一个用双线性插值的方法来获取某一卷积形变后位置的输入的过程。

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{DLTips/DeformableConv1.jpg}
\caption{Deformable Convolution的实现示意图}
\label{DeformableConv1}
\end{figure}

应用：
\begin{itemize}
\item Deformable Convolutional Networks
\end{itemize}

可能 会跟目标检测、跟踪相结合。

\subsection{特征重标定卷积}

在ImageNet2017比赛中，冠军模型SENet的核心模块，被称为"Squeeze-and-Excitation"， 知乎的作者把它就先成为特征重标定卷积了。

和前面的不同，本文提出的算法的出发点在于改进特征为度，包括卷积核的数量等。现在一个卷积层中，还不是整个神经网络，有数以千计的卷积核，而且我们知道每一种卷积核对应提取一种特征，但得到这么多的特征，肯定有一些是更重要的，有一些是不那么重要的。所以本文的方法是通过学习方式来自动获取每个特征通道的重要程度，然后按照计算出来的重要程度去提升有用的特征并抑制对当前任务用处不大的特征。

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{DLTips/SENetConv0.jpg}
\caption{Squeeze-and-Excitation Block示意图}
\label{SENetConv0}
\end{figure}

虽然听上去很复杂，但实现起来却比较简单？步骤如下：
\begin{itemize}
\item 首先对输入$X$做常规的卷积$F_{tr}$，得到一个Output Feature Map ($U$)，它的大小为$C, H, W$， 文章的作者认为，得到的这个Output是非常混乱的。
\item 为了得到这些Feature Maps(共$C$个)的重要程度，直接对这些Feature Map做一个Global Average Pooling\footnote{这里还涉及到一个额外的东西，如果你了解卷积，你就会发现一旦某一特征经常被激活，那么Global Average Pooling计算出来的值会比较大，说明它对结果的影响也比较大，反之越小的值，对结果的影响就越小。}， 然后就得到一个长度为$C$的向量，注意是向量了！ 在图\ref{SENetConv0}中表现就是由$U$经操作$F_{sq}(\cdot)$得到$1 \times 1 \times C$的过程。

\item 然后我们对这个向量加两个FC层，做非线性映射，这俩FC层的参数，也就是网络需要额外学习的参数。对应图中$F_{ex}(\cdot, W)$操作。

\item 最后输出的向量，我们可以看做特征的重要性程度，然后与feature map对应channel相乘就得到特征有序的feature map了。对应图中$F_{scale}(\cdot, \cdot)$操作。
\end{itemize}

应用：
\begin{itemize}
\item Squeeze-and-Excitation Networks
\item 另外它还可以和几个主流网络结构结合起来一起用，比如Inception和Res。图\ref{SENetConv1}所示。
\end{itemize}

\begin{figure}[!htbp]
\centering
\subfigure[SE-Inception Module]{\label{SEInception0}
\includegraphics[width=0.45\textwidth]{DLTips/SEInception0.jpg}
}
\:
\subfigure[SE-ResNet Module]{\label{SEResNet0}
\includegraphics[width=0.45\textwidth]{DLTips/SEResNet0}
}
\caption{SENet与Inception以及ResNet结合的示意图}
\label{SENetConv1}
\end{figure}

\subsection{小结-比较}

图\ref{Conv0}是对上面提到的不同的卷及类型的一个比较与总结。

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{DLTips/Conv0.jpg}
\caption{不同卷积策略的比较}
\label{Conv0}
\end{figure}

我们把图像（height，width）作为空间维度，把channels做为特征维度。

\section{全连接与卷积的异同}
全连接：Fully Connected

注意这里与FCN中的FC不同，FCN里面的FC是Fully Convolution,所以还是卷积操作，所以才有用Fully Convolution代替Fully Connected之说。

Fully Connected的作用是什么？其本质上还是一个卷积运算，只不过输入卷积核的大小跟最后一层Feature Map的大小一致， 所以得到的结果是一个标量。其实就是对前面CNN网络提取的特征进行变换，对这些Feature maps进行组合，得到目标类的一个代表值，输入Sigmoid函数进行计算，得到分类结果。但这种方式参数非常多，因为在实现Fully Connected时需要根据最后一层Feature Map的大小来确定计算的卷积核，这也导致了它需要解决不同输入大小时候的数据问题。

一个简单的例子，输入是$228 * 228 * 3$，然后最后一层Feature Map的维度是$7 * 7 * 512$，即，共包含512个Feature Maps，每一个Feature Map的大小是$7 * 7$，那么全连接层需要这样设计，假设我们想要输出1024个分类，那么全连接的参数的数量就是：
$7 * 7 * 512 * 1024$，所以参数非常多！另一方面，如果输入不是$228 * 228 * 3$而是$456 * 456 * 3$那么，得到的最后一层Feature map的大小也不是$7 * 7 * 512$，现在而是$14 * 14 * 512$，那么全连接也需要改变，变成$14 * 14 * 512 * 1024$, 这样非常不灵活。

因为参数实在太多，所以现在在最后为了得到Sigmoid函数的输入(标量)，都选择使用Global Average Pooling来代替全连接。Global Average Pooling也就是说用一个Feature Map的平均值作为Sigmoid输入，输出为类别信息。

不过，也有研究人员表明，全连接层有助于在微调(Fine-tune)过程中进行知识迁移，尤其源领域与目标领域很不一样的时候，更是如此。

\section{Pooling}

\subsubsection{Global Average Pooling}

就是对一个Feature Map进行加和求平均值。具体的应用可参考上一小节。

\subsubsection{Unpooling}

而上池化的实现主要在于池化时记住输出值的位置，在上池化时再将这个值填回原来的位置，其他位置填0即OK。(参考：SegNet, DeconvNet)

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.75\textwidth]{DLTips/Unpooling0.png}
\caption{Unpooling示意图}
\label{Unpooling0}
\end{figure}

图\ref{Unpooling0}可以认为额外的\verb|switch variables|实现了保存Pooling过程中的位置。

In the convnet, the max pooling operation is non-invertible, however we can obtain an approximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus.

也就是说用一组开关变量保存最大值在Pooling Region中的位置。

参考文献： \href{https://www.quora.com/What-is-the-difference-between-Deconvolution-Upsampling-Unpooling-and-Convolutional-Sparse-Coding}{Quora Answer}

\section{Local Response Normalization}

Reference: \href{https://prateekvjoshi.com/2016/04/05/what-is-local-response-normalization-in-convolutional-neural-networks/}{What is LRN}

为什么需要设置Normalization Layers?
Anyway, the reason we may want to have normalization layers in our CNN is that we want to have some kind of inhibition scheme. 这个Inhibition Scheme是什么意思。

侧边抑制：ateral inhibition。 一个激活的神经会抑制旁边神经的激活。

\subsubsection{到底什么是LRN}

Local Response Normalization (LRN) layer implements the lateral inhibition we were talking about in the previous section. This layer is useful when we are dealing with ReLU neurons. Why is that? Because ReLU neurons have unbounded activations and we need LRN to normalize that. We want to detect high frequency features with a large response. If we normalize around the local neighborhood of the excited neuron, it becomes even more sensitive as compared to its neighbors.

At the same time, it will dampen(抑制) the responses that are uniformly large in any given local neighborhood(值普遍很大的局部). If all the values are large, then normalizing those values will diminish all of them. So basically we want to encourage some kind of inhibition and boost the neurons with relatively larger activations. 

\subsubsection{如何实现LRN}

There are two types of normalizations available in Caffe. You can either normalize within the same channel or you can normalize across channels. Both these methods tend to amplify the excited neuron while dampening the surrounding neurons. When you are normalizing within the same channel, it’s just like considering a 2D neighborhood of dimension N x N, where N is the size of the normalization window. You normalize this window using the values in this neighborhood. If you are normalizing across channels, you will consider a neighborhood along the third dimension but at a single location. You need to consider an area of shape N x 1 x 1. Here 1 x 1 refers to a single value in a 2D matrix and N refers to the normalization size.

在 AlexNet 中Normalized的计算公式如下：

\begin{displaymath}
b_{x, y}^i = a_{x, y}^i / \left( k + \alpha \sum_{j = \max(0, i - n/2)}^{\min(N-1, i + n/2)(a_{x, y}^j)^2} \right)
\end{displaymath}

其中, $a_{x, y}^i, b_{x, y}^i$分别是输入的激活值，输出的Normalized的值。

\section{CNN中感受野的计算}

参考文献：

[1] \href{https://blog.csdn.net/kuaitoukid/article/details/46829355}{CNN中感受野的计算-CSDN}

[2] \href{https://www.cnblogs.com/objectDetect/p/5947169.html}{CNn中感受野的计算-博客园}

感受野（receptive field）是怎样一个东西呢，从CNN可视化的角度来讲，就是输出featuremap某个节点的响应对应的输入图像的区域就是感受野。

比如我们第一层是一个3*3的卷积核，那么我们经过这个卷积核得到的featuremap中的每个节点都源自这个3*3的卷积核与原图像中3*3的区域做卷积，那么我们就称这个featuremap的节点感受野大小为3*3

如果再经过pooling层，假定卷积层的stride是1，pooling层大小2*2，stride是2，那么pooling层节点的感受野就是4*4

有几点需要注意的是，padding并不影响感受野，stride只影响下一层featuremap的感受野，size影响的是该层的感受野。

具体计算时，需要：
\begin{itemize}
\item 第一层卷积层的输出特征像素的感受野的大小就是滤波器的大小
\item 深层卷积层的感受野大小和它之前的所有层的滤波器大小和步长有关系
\item 计算感受野大小时，忽略图像边缘的影响，即不考虑Padding的大小。
\end{itemize}

关于感受野的计算，多采用Top to Down的方式，即最先计算最深层的前一层的感受野，然后逐渐传递到第一层，使用的公式如下：
\begin{enumerate}
\item RF = 1   // 待计算的Feature Map的感受野大小
\item For layer in (top layer to down layer):
\item      RF = ((RF - 1) * stride) + fsize
\end{enumerate}

stride 表示卷积的步长； fsize表示卷积层滤波器的大小。

\subsubsection{具体的例子}

%\begin{table}
\begin{tabular}{ccc}
\toprule
type & size & stride \\
\midrule
conv1 & 3 & 2 \\
pool1 & 2 & 2 \\
conv2 & 3 & 1 \\
pool2 & 2 & 2 \\
conv3 & 3 & 1 \\
conv4 & 3 & 1 \\
pool3 & 2 & 2 \\
\bottomrule
\end{tabular}
%\end{table}

pool3的一个输出对应pool3的输入大小为2*2

感受野计算如下：

依次类推，对应conv4的输入为4*4，因为2*2的每个角加一个3*3的卷积核，就成了4*4，当然这是在stride=1的情况下才成立的，但是一般都是stride=1，不然也不合理

对应conv3的输入为6*6

对应pool2的输入为12*12

对应conv2的输入为14*14

对应pool1的输入为28*28

对应conv1的输入为30*30

所以pool3的感受野大小就是30*30

\subsubsection{Stride的计算}

每一个卷积层有一个Strides的概念，这个Strides就是之前所有层Stride的乘积。即
\begin{displaymath}
strids(i) = stride(1) * stride(2) * stride(3) * \ldots * stride(i - 1)
\end{displaymath}

\subsubsection{专业的计算}

参考文献：\href{https://zhuanlan.zhihu.com/p/26663577}{卷积神经网络中的感受野计算(译)-知乎}， \href{https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807}{原文-英语}

定义：The receptive field is defined as the region in the input space that a particular CNN’s feature is looking at (i.e. be affected by).

这篇英语文章，主要是四个公式：
\begin{itemize}
\item Calculate the number of output features

\begin{displaymath}
n_{out} =	\lfloor \frac{n_{in} + 2p - k}{s} \rfloor + 1
\end{displaymath}

其中，$p$表示单侧Padding大小， $k$表示filter kernel的大小。


\item Calculate the Jump (Strides) in the output feature map

\begin{displaymath}
j_{out} = j_{in} * s
\end{displaymath}

其中， $j$表示Strides，注意stride是不断积累的。如上下文中提到的。

\item Calculate the receptive field size

\begin{displaymath}
r_{out} = r_{in} + (k - 1) * j_{in}
\end{displaymath}

$k$表示kernel filter的大小。

\item Calculate the center position of the receptive field of the first output feature

\begin{displaymath}
start_{out} = start_{in} + (\frac{k - 1}{2} - p) * j_{in}
\end{displaymath}

\end{itemize}

The first layer is the input layer, which always has n = image size, r = 1, j = 1, and start = 0.5. 

小结如图\ref{PerceptionField1}所示。
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{DLTips/PerceptionField1.png}
\caption{文章的总结与说明}
\label{PerceptionField1}
\end{figure}


\href{https://zhuanlan.zhihu.com/p/28492837}{神经网络中的感受野-知乎}

由于图像是二维的，具有空间信息，因此感受野的实质其实也是一个二维区域。但业界通常将感受野定义为一个正方形区域，因此也就使用边长来描述其大小了。在接下来的讨论中，本文也只考虑宽度一个方向。我们先按照图\ref{PerceptionField0}所示对输入图像的像素进行编号。

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.75\textwidth]{DLTips/PerceptionField0.jpg}
\caption{二维图像像素编号示意图}
\label{PerceptionField0}
\end{figure}

接下来我们使用一种并不常见的方式来展示CNN的层与层之间的关系（如下图，请将脑袋向左倒45°观看$>\_<$），并且配上我们对原图像的编号。

图中黑色的数字所构成的层为原图像或者是卷积层，数字表示某单元能够看到的原始图像像素。我们用 $r_n$ 来表示第 n 个卷积层中，每个单元的感受野（即数字序列的长度）；蓝色的部分表示卷积操作，用 $k_n$ 和 $s_n$ 分别表示第 n 个卷积层的kernel\_size和stride。

对Raw Image进行kernel\_size=3, stride 2的卷积操作所得到的fmap1 (fmap为feature map的简称，为每一个conv层所产生的输出)的结果是显而易见的。序列[1 2 3]表示fmap1的第一个单元能看见原图像中的1，2，3这三个像素，而第二个单元则能看见3，4，5。这两个单元随后又被kernel\_size=2，stride 1的Filter 2进行卷积，因而得到的fmap2的第一个单元能够看见原图像中的1,2,3,4,5共5个像素（即取[1 2 3]和[3 4 5]的并集）。

接下来我们尝试一下如何用公式来表述上述过程。可以看到，[1 2 3]和[3 4 5]之间因为Filter 1的stride 2而错开（偏移）了两位，而3是重叠的。对于卷积两个感受野为3的上层单元，下一层最大能获得的感受野为 $3\times2=6$ ，但因为有重叠，因此要减去(kernel\_size - 1)个重叠部分， Kernel的size是本层的Kernel的尺寸，而重叠部分的计算方式则为上一层的感受野减去前面所说的本层的偏移量，这里是2. 因此我们就得到 $r_2=r_1\times k_2-(r_1-s_1)\times(k_2-1)=3\times2-(3-2)\times(2-1)=5$ 。

继续往下一层看，我们会发现[1 2 3 4 5]和[3 4 5 6 7]的偏移量仍为2，并不简单地等于上一层的 $s_2$ ，\textbf{这是因为之前的stride对后续层的影响是永久性的}，而且是累积相乘的关系（例如，在fmap3中，偏移量已经累积到4了），也就是说 $r_3$ 应该这样求 

\begin{displaymath}
r_3=r_2\times k_3-(r_2-s_1\times s_2)\times(k_3-1)=5\times3-(5-2)\times(3-1)=9 
\end{displaymath}

以此类推，

\begin{displaymath}
r_4=r_3\times k_4-(r_3-s_1\times s_2\times s_3)\times(k_4-1)=9\times2-(9-4)\times(2-1)=13 
\end{displaymath}


于是我们就可以得到关于计算感受野的抽象公式了： 

\begin{displaymath}
r_n=r_{n-1}\times k_n-(r_{n-1}-\prod_{i=1}^{n-1}s_i)\times(k_n-1) 
\end{displaymath}


经过简单的代数变换之后，最终形式为：

\begin{displaymath}
r_n=r_{n-1}+(k_n-1)\prod_{i=1}^{n-1}s_i 
\end{displaymath}

这个公式也体现了上文提到的stride影响的下一层。


\section{神经网络中的初始化}

\subsubsection{回答一}

参考文献：\href{https://www.zhihu.com/question/56526007}{为什么要进行初始化-知乎}

参数初始化的目的是为了让神经网络在训练过程中学习到有用的信息，这意味着参数梯度不应该为0。所以参数初始化应该满足两个条件：
\begin{itemize}
\item 各个激活层不会出现饱和现象，比如对于Sigmoid激活函数，初始化值不能太大也不能太小，导致进入饱和区
\item 各个激活值不为0，如果激活层输出为0，也就是下一层的输入为0， 所以这个卷积层对权重的偏导数为0， 那么导致梯度也为0
\item 另一个非常重要的原因，就是Xavier， MSRA等这些初始化不至于一开始就让网络发散，或者梯度消失
\end{itemize}

所以，最主要要注意两个问题，首先是梯度不能消失，然后网络不能发散。

\subsection{Xavier}

参考文章：\href{https://zhuanlan.zhihu.com/p/27919794}{深度前馈网络与Xavier初始化原理-知乎}

而为什么把模型的参数初始化成全0就不行了呢？这个不用讲啦，全0的时候每个神经元的输入和输出没有任何的差异，换句话说，根据前面BP算法的讲解，这样会导致误差根本无法从后一层往前传（乘以全0的$\omega$后误差就没了），这样的model当然没有任何意义。

{\bfseries 那么我们不把参数初始化成全0，那我们该初始化成什么呢？换句话说，如何既保证输入输出的差异性，又能让model稳定而快速的收敛呢？}

要描述“差异性”，首先就能想到概率统计中的方差这个基本统计量。对于每个神经元的输入z这个随机变量，根据前面讲BP时的公式，它是由线性映射函数得到的,也就是：
\begin{displaymath}
z = \sum_{i = 1}^{n}\omega_i x_i
\end{displaymath}

其中n是上一层神经元的数量。因此，根据概率统计里的两个随机变量乘积的方差展开式

\begin{displaymath}
Var(\omega_i x_i) = E[\omega_i]^2 Var(x_i) + E[x_i]^2Var(\omega_i) + Var(\omega_i)Var(x_i)
\end{displaymath}

所以，如果$E(x_i) = E(\omega_i)=0$(可以通过批量归一化Batch Normlization来满足，其它大部分情况也不会差太多)，　那么就有：
\begin{displaymath}
Var(z) = \sum_{i = 1}^{n}Var(x_i)Var(\omega_i)
\end{displaymath}

如果变量$x_i$与$\omega_i$满足独立同分布的话：
\begin{displaymath}
Var(z) = n \cdot Var(x)Var(\omega)
\end{displaymath}

好了，这时重点来了。试想一下，根据文章《激活函数》，整个大型前馈神经网络无非就是一个超级大映射，将原始样本稳定的映射成它的类别。也就是将样本空间映射到类别空间。试想，如果样本空间与类别空间的分布差异很大，比如说类别空间特别稠密，样本空间特别稀疏辽阔，那么在类别空间得到的用于反向传播的误差丢给样本空间后简直变得微不足道，也就是会导致模型的训练非常缓慢。同样，如果类别空间特别稀疏，样本空间特别稠密，那么在类别空间算出来的误差丢给样本空间后简直是爆炸般的存在，即导致模型发散震荡，无法收敛。因此，我们要让样本空间与类别空间的分布差异（密度差别）不要太大，也就是要让它们的方差尽可能相等。这里的样本空间与类别空间可以理解成是输入$x$空间与输出$z$的空间，以及其所对应的方差。

如果需要两个空间的方差差异不大，即满足$Var(z) = Var(x)$，　那么需要$n \cdot \omega = 1$，　也就是说$Var(\omega) = 1 / n$。其中$n$表示神经元输出端对应的个数。在前向传播时，对于特定一层神经网络，$n = n_{in}$，　在后向传播时，$n = n_{out}$而一般这两者都不相同，因此可以取它们的中间值：
\begin{displaymath}
Var(\omega) = \frac{1}{\frac{n_{in} + n_{out}}{2}}
\end{displaymath}

假设$\omega$均匀分布时，由$\omega$在区间$[a, b]$内均匀分布的方差为：
\begin{displaymath}
Var = \frac{(b - a)^2}{12}
\end{displaymath}

联立上面两个公式，可以得出$\omega$的分布区间(假设$b = -a$)：
\begin{displaymath}
\omega \sim U\left[ -\frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}  \right]
\end{displaymath}
（让w在这个区间里均匀采样就好啦）

得到的这个结论就是Xavier初始化方法。这就是为什么使用Xavier初始化这个trick经常可以让model的训练速度和分类性能取得大幅提高啦~所以在使用前馈网络时，除非你的网络设计的明显不满足xavier的假设，否则使用xavier往往不会出错。当然，另一方面来说，也很少有场合可以完全迎合xavier假设，因此时间充裕的话，改改分子，甚至去掉$n_{out}$都有可能带来意想不到的效果。


\section{计算图的后向传播计算}

参考文章：

[1] \href{https://zhuanlan.zhihu.com/p/27919794}{深度前馈网络与Xavier初始化原理-知乎}

[2] \href{https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247484344&idx=1&sn=92ff388d93088ea6461023b376860ce7&chksm=970c2b6ea07ba2782da5de78b23b78b267b4b0fc4454e76256134cfc9d6aa5416e0bd4c14097&scene=21#wechat_redirect}{BP算法－夕小瑶}

\subsubsection{简单的说明}
误差反向传播过程， 其本质就是基于链式求导 + 梯度下降。

首先往上翻一翻，记住之前说过的前馈网络无非就是反复的线性与非线性映射。

首先，假设某个神经元的输入为z，经过激活函数$f_1(\cdot)$得到输出a。即函数值$a=f_1(z)$。如果这里的输入z又是另一个函数f2的输出的话（当然啦，这里的$f_2$就是线性映射函数，也就是连接某两层的权重矩阵），即$z=f_2(x)$，那么如果基于a来对z中的变量x求导的时候，由于 

\begin{displaymath}
\frac{\partial a}{\partial x} = \frac{\partial a}{\partial z}\cdot \frac{\partial z}{\partial x} = f_1^{'}(z)\frac{\partial z}{\partial x}
\end{displaymath}

上式也就是链式求导。更一般的链式求导法则为：
\begin{displaymath}
\frac{\partial a}{\partial x} = \sum_{i} \frac{\partial a}{\partial z_i}\cdot \frac{\partial z_i}{\partial x}
\end{displaymath}

显然只要乘以激活函数f1的导数，就不用操心激活函数的输出以及更后面的事儿了（这里的“后面”指的是神经网络的输出端方向），只需要将精力集中在前面的东西，即只需要关注z以及z之前的那些变量和函数就可以了。因此，误差反向传播到某非线性映射层的输出时，只需要乘上该非线性映射函数在z点的导数就算跨过这一层啦。

而由于$f2(\cdot)$是个线性映射函数，即 $f_2(x)=\omega \cdot x + b$，因此
\begin{displaymath}
\frac{\partial z}{\partial x} = \omega
\end{displaymath}
因此，当误差反向传播到线性映射层的输出时，若想跨过该层，只需要乘以线性映射函数的参数就可以啦~即乘上$\omega$。

而这里的x，又是更前面的非线性映射层的输出，因此误差在深度前馈网络中反向传播时，无非就是反复的跨过非线性层和线性层，也就是反复的乘以非线性函数的导数(即激活函数的导数)和线性函数的导数（即神经网络的参数/权重/连接边）。

也就是下面这张图啦（从右往左看）：

\begin{figure}[!bthp]
\centering
\includegraphics[width=0.75\textwidth]{DLTips/BP0.jpg}
\caption{BP计算过程示意图}
\label{BP0}
\end{figure}

\subsection{Notes on CNNs}

参考文献：\href{https://zhuanlan.zhihu.com/p/29458808}{CNN反向求导理解-知乎}

\subsubsection{CNN的目标函数，包含Weight Decay的形式}

假设共有m个样本$(x_i, y_i)$，CNN网络共有L层，中间包含若干卷积层和Pooling层，最后一层的输出为$f(x_i)$，那么目标函数是(交叉熵的形式):
\begin{displaymath}
Loss = -\frac{1}{m}\sum_{i=0}^{m-1}y_i^{'}\log f(x_i) + \lambda \sum_{k=1}^{L}\parallel W_k \parallel^2
\end{displaymath}
其中， $W_k$为每一层卷积层的权重。

\subsubsection{求解输出层的误差敏感项}

现在只考虑一个输入样本(x, y）的情形，loss函数和上面的公式类似，使用交叉熵来表示的，暂时不考虑权值规则项，样本标签采用One-Hot编码，CNN网络的最后一层采用SoftMax输出，样本(x, y)经过CNN网络后，输出用f(x)表示，则对应该样本的loss是：
\begin{displaymath}
l(f(\mathbf{x}, y)) = -\sum_{c}1_{(y=c)} \log f(x)_c = -\log f(x)_y
\end{displaymath}

其中， $f(x)_c = p(y = c|x)$。因为x通过CNN后得到的输出f(x)是一个向量，该向量的元素值都是概率值，分别代表着x被分到各个类中的概率，而f(x)中下标c的意思就是输出向量中取对应c那个类的概率值。

采用上面的符号，可以求得此时loss值对输出层的误差敏感性表达式为：
\begin{displaymath}
\nabla_{\mathbf{a}^{(L+1)(\mathbf{x})}} - \log f(\mathbf{x})_y = -(\mathbf{e}(y) - f(x))
\end{displaymath}

其中, $\mathbf{a}^{(L+1)(\mathbf{x})}$是最后一层卷积的输出，有$f(x) = softmax(\mathbf{a}^{(L+1)(\mathbf{x})})$, $e(y)$为样本x标签值的one-hot表示，其中只有一个元素是1， 其它的都为0。


推到如下：
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.85\textwidth]{DLTips/BP1.jpg}
\caption{目标函数对最后一层卷积层的输出，即最后一层Softmax的输入的求导}
\label{BP1}
\end{figure}

注意的是，是对任意一个$f(x)_c$求到的过程，但目标函数是根据正确的标签来确定的，也就是$-\log f(x)_y$中的y与分母中的c是不同的！前者是标签，后者是Softmax的第c个输入。

由上面公式可知，如果输出层采用sfotmax，且loss用交叉熵形式，则最后一层的误差敏感值就等于CNN网络输出值f(x)减样本标签值e(y),即f(x)-e(y),其形式非常简单，这个公式是不是很眼熟？很多情况下如果model采用MSE的loss，即$loss=1/2*(e(y)-f(x))^2$，那么loss对最终的输出f(x)求导时其结果就是f(x)-e(y),虽然和上面的结果一样，但是大家不要搞混淆了，这2个含义是不同的，一个是对输出层节点输入值的导数(softmax激发函数)，一个是对输出层节点输出值的导数(任意激发函数）。而在使用MSE的loss表达式时，输出层的误差敏感项
为$(f(x)-e(y)).*f(x)'$，两者只相差一个因子。

这样就可以求出第L层的权值W的偏导数：

\begin{displaymath}
\frac{\partial Loss}{\partial W_L} = \frac{\partial Loss}{\partial (-\log f(x)_y)} \cdot \frac{f(x)}{\partial W} + \lambda W_L
\end{displaymath}

对输出偏置层的偏导数是：
\begin{displaymath}
\frac{\partial Loss}{\partial b_L} = -\frac{1}{m} * (e(y) - f(x))
\end{displaymath}

\subsubsection{当卷积层的下一层是Pooling层时，求解卷积层的误差敏感项}

若第l层为卷积层，第l+1层为Pooling层，且Pooling层的误差敏感项是：
\begin{displaymath}
\delta_j^{l+1}
\end{displaymath}
卷积层的误差敏感项为：
\begin{displaymath}
\delta_j^l
\end{displaymath}

那么两者的关系是：
\begin{displaymath}
\delta_j^l = upsample(\delta_j^{l+1}) \cdot h(\delta_j^l)'
\end{displaymath}

其中， $\cdot$ 是矩阵的点积，即对应元素的乘积。卷积层与Upsample后的Pooling的输出层的节点是一一对应的，所以可以都用下标j表示。后面的符号：
$h(\delta_j^l)'$表示的是第l层第j个节点处激活函数的导数，是对节点输入的导数？也就是第l层卷积层的第j个节点。

那么问题来了，重点就是这个Upsample怎么实现了！而具体的形式是与采用什么Pooling方法有关的。但unsample的大概思想为：pooling层的每个节点是由卷积层中多个节点(一般为一个矩形区域)共同计算得到，所以pooling层每个节点的误差敏感值也是由卷积层中多个节点的误差敏感值共同产生的，只需满足两层见各自的误差敏感值相等，下面以mean-pooling和max-pooling为例来说明。

　假设卷积层的矩形大小为4×4, pooling区域大小为$2×2$, 很容易知道pooling后得到的矩形大小也为2*2（本文默认pooling过程是没有重叠的，卷积过程是每次移动一个像素，即是有重叠的，后续不再声明）,如果此时pooling后的矩形误差敏感值如下：
　
\begin{figure}[!htp]
\centering
\subfigure[输出的2*2Pooling结果]{
\includegraphics[width=0.45\textwidth]{DLTips/BP2.png}
}
\;
\subfigure[输入的4*4卷积层输出]{
\includegraphics[width=0.45\textwidth]{DLTips/BP3.jpg}
}
\caption{则按照mean-pooling，首先得到的卷积层应该是4×4大小，其值分布为(等值复制).}
\end{figure}

因为得满足反向传播时各层间误差敏感总和不变，所以卷积层对应每个值需要平摊（除以pooling区域大小即可，这里pooling层大小为2×2=4)），最后的卷积层值

分布为：

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.65\textwidth]{DLTips/BP4.jpg}
\caption{反向传播时的误差}
\end{figure}

mean-pooling时的unsample操作可以使用matalb中的函数kron()来实现，因为是采用的矩阵Kronecker乘积。C=kron(A, B)表示的是矩阵B分别与矩阵A中每个元素相乘，然后将相乘的结果放在C中对应的位置。

上面说的是Avg Pooling的计算，下面说一下MaxPooling的计算：

需要记录前向传播过程中pooling区域中最大值的位置，这里假设pooling层值1,3,2,4对应的pooling区域位置分别为右下、右上、左上、左下。则Upsample时，在相应的位置放置误差敏感项，其余的位置填0.

\subsubsection{当Pooling的下一层为卷积层时，求该Pooling层的误差敏感项}

假设第l层(pooling层)有N个通道，即有N张特征图，第l+1层(卷积层)有M个特征，l层中每个通道图都对应有自己的误差敏感值，其计算依据为第l+1层所有特征核的贡献之和。

下面是第l+1层中第j个核对第l层中第i个通道的误差敏感值计算方法：
\begin{displaymath}
\delta_i^l = \sum_{j=1}^{M}\delta_j^{l+1} \star K_{ij}
\end{displaymath}
其中，$\star$是矩阵的卷及操作。

如果不明白，或者更多的内容，还是看原文吧。








\subsection{Pooling层的反向传播}

前面一直关注了正向卷积的求导问题了，现在应该关注下Pooling层的求导问题！







\section{神经网络中的Attention机制}

参考文献：\href{https://zhuanlan.zhihu.com/p/35571412}{浅谈Attention机制-知乎}

\subsection{Recurrent Models of Visual Attention}

参考文献：\cite{Attention2014}

Our model considers attention-based processing of a visual scene as a control problem and is general enough to be applied to static images, videos,
or as a perceptual module of an agent that interacts with a dynamic visual environment (e.g. robots,
computer game playing agents).

Instead of processing an entire image or even bounding box at once, at each step, the model
selects the next location to attend to based on past information and the demands of the task.

We describe an end-to-end
optimization procedure that allows the model to be trained directly with respect to a given task and
to maximize a performance measure which may depend on the entire sequence of decisions made by
the model. This procedure uses backpropagation to train the neural-network components and policy
gradient to address the non-differentiabilities due to the control problem

目标检测的几种方法：
\begin{itemize}
\item 基于窗口的分类器，包括Proposal等, 计算量大
\item 基于Saliency Detection的， 不能Interate information across fixations, 仅利用底层图像特征， 忽略了Semantic Content of a scene and task demands.
\item 一些工作把视觉问题当做Sequential decision task，本文也是
\item 
\end{itemize}

Our formulation which employs an RNN to integrate visual
information over time and to decide how to act is, however, more general, and our learning procedure
allows for end-to-end optimization of the sequential decision process instead of relying on greedy
action selection.

我们的模型，既可以实现性质图像的Object Recognition, 而且还适用于动态环境，以一种Task-driven的方式。

\subsubsection{The Recurrent Attention Model - RAM }

In this paper we consider the attention problem as the sequential decision process of a goal-directed
agent interacting with a visual environment.

这篇文章里面，感觉这意思，Attention机制不就是Reinforcement Learning么？

该公式包含了各种任务，如静态图像中的对象检测， 控制问题，即根据屏幕上的图像流来学习打游戏等。

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{DLTips/Attention0.png}
\caption{Attention 模型示意图}
\label{Attention0}
\end{figure}

图\ref{Attention0}中，$x_t$表示第$t$时刻的输入图像，$l_{t-1}$表示位置，$\rho (x_t, l_{t-1})$表示这个位置的Retina-like representation. $g_t$为glimpse representation。 

We will refer to this low-resolution representation
as a glimpse。

Action分为两个部分：
\begin{itemize}
\item Location actions

are chosen stochastically from a distribution parameterized by the location network $f_l(h_t; \theta_t)$ at time $t$。

\item Environment action

The environment action a t is similarly drawn from a distribution conditioned
on a second network output $a_t \sim p(\cdot | f_a(h_t; \theta_a))$

\end{itemize}

与RL里面的Partially Observable Decision Process (POMDP)的一个特例。

\subsubsection{总结}

感觉本文中的Attention与RL关系密切，而且与RNN网络结合起来了。那么到底什么是Attention机制，还需要阅读其它文章，现在我还不太明白。


\section{小型网络}

参考文章：\href{https://zhuanlan.zhihu.com/p/37074222}{CVPR2018高效小网络-知乎}

\subsection{理论分析}

先说结论：卷积层主要引入了计算量，全连接层主要引入了参数数量。

参数数量用params表示，关系到模型大小，单位通常为M，通常参数用float32表示，所以模型大小是参数数量的4倍。

理论计算量用FLOPs或者M-Adds表示，这里用FLOPs写起来简单，关系到算法速度，大模型的单位通常为G，小模型通道为M。

需要注意的两点：
\begin{itemize}
\item 理论计算量通常只考虑乘加操作(Multi-Adds)的数量，而且只考虑CONV和FC等参数层的计算量，忽略BatchNorm和PReLU等等。一般情况，CONV和FC层也会忽略仅纯加操作的计算量，如bias偏置加和shotcut残差加等，\textbf{目前技术有BN的CNN可以不加bias}。

\item 理论计算量通常和实际ARM实测速度会有不一致，主要是理论计算量太过理论化，没有考虑不同硬件IO速度和计算力差异，最重要的是inference framework部署框架优化水平和程度差异，不同框架的优化的关注点不一样，同一框架对不同层的优化程度也不一样。Inference Framework以我用过的ncnn为代表。
\end{itemize}

假设，卷积核大小为$k_h * k_w$， 输入的特征的通道为$C_{in}$,输出的Feature Map的数量为$C_{out}$, Feature Map的尺寸是$H*W$。那么，\textbf{卷积层}对应的参数量\textit{params}和计算量\textit{FLOPs}分别是：

\begin{displaymath}
\begin{gathered}
\#Params: (k_h * k_w * c_{in} + 1) * c_{out} \\
\#FLOPs: k_w * k_h * c_{in} * c_{out} * W * H
\end{gathered}
\end{displaymath}

相对比的，\textbf{全连接层}对应的上述两个参数分别是：
\begin{displaymath}
\begin{gathered}
\#Params: (c_{in} + 1) * C_{out} \\
\#FLOPs: c_{in} * c_{out}
\end{gathered}
\end{displaymath}

\subsection{GCONV \& DWCONV}

有时间在整理吧，细节见原文。

结论就是，通过输入通道的分组，可以降低参数量以及计算量$g$倍，其中$g$是输入通道的分组数，并且在DWCONV中这个分组数$g$就是输入的通道数$C_{in}$。

其它的一点说明：

设计CNN目前都采用堆block的方式，后面对每个模型只集中分析其block的设计。堆Block简化了网络设计问题：只要设计好block结构，对于典型224 $\times$ 224输入，设计CNN只需要考虑112$\times$112、56$\times$56、28$\times$28、14$\times$14、7$\times$7 这5个分辨率阶段(stage)的block数量和通道数量就可以了。

CNN加深度一般都选择14$\times$14这个分辨率阶段，多堆几个block，这一层的参数数量和计算量占比最大，所以我们选这一层作为特例分析每种block的参数数量和计算量，量化说明选择$14\times14\times512$的输入和输出情况。

\subsection{NiN及相关}




\section{Deep Reinforcement Learning}

参考文献：\href{https://deeplearning4j.org/deepreinforcementlearning}{A Beginner's Guide to Deep Reinforcement Learning}

几个重要的摘抄。
\begin{itemize}
\item Algorithms can start from a blank slate, and under the right conditions they achieve superhuman performance. Like a child incentivized by spankings and candy, these algorithms are penalized when they make the wrong decisions and rewarded when they make the right ones – this is reinforcement.

\item Reinforcement learning solves the difficult problem of correlating immediate actions with the delayed returns they produce. 
\end{itemize}

\section{为什么Python中要继承object类}

参考文章：\href{https://stackoverflow.com/questions/4015417/python-class-inherits-object}{stackoverflow answer}

在Python3中，除了因为要兼容于Python2，没有其它原因。而在Python2中，则有些复杂。

\subsubsection{Python 2.x story}
Python2(从python2.2开始)中，有两种类型的类(two styles of classes), 他们根据是否继承自\textbf{object}来区分。

\begin{itemize}
\item Classic style classes, they don't have object as a base class
\item New style classes: they have, directly or indirectly (\textit{e.g.} inherit from a built-in type), \textbf{object} as a base class
\end{itemize}

那么为什么选择用新式类呢(New style classes)?几个原因如下：
\begin{itemize}
\item Support for descriptors. Sepcifically, the following constructs are made possible with descriptors:
\begin{itemize}
\item classmethod
\item staticmethod
\item properties with property
\item \_\_slots\_\_
\end{itemize}
\item the \verb|__new__| static method
\item method resolution order (MRO)
\item Related to MRO
\end{itemize}

\subsubsection{Python 3.x story}

In Python 3, things are simplified. Only new-style classes exist (referred to plainly as classes) so, the only difference in adding object is requiring you to type in 8 more characters. 


\subsubsection{Choose which one}

\begin{itemize}
\item \textbf{In Python 2.x}, \textbf{always} inherit from \textbf{object} explicitly
\item \textbf{In Python 3.x}, inherit \textbf{object} if you are writing code that tries to be Python agnostic (不可知论者), that is, it needs to work both in python 2 and in python 3. Otherwise, don't, it really makes no difference since Python inserts it for you behind the scenes.
\end{itemize}

\section{1*1卷积}

参考文章：\href{https://www.zhihu.com/question/56024942}{1*1卷积的作用与好处-知乎}

1*1卷积 和正常的卷积一样，唯一不同的是它的大小是1*1，没有考虑在前一层局部信息之间的关系， 所以这里跟普通说卷积的时候一样，都是自动忽略了在Channel上的维度，而只是空间域的维度是1*1或3*3或5*5等。最早出现在 Network In Network的论文中 ，使用1*1卷积是想加深加宽网络结构 ，在Inception网络（ Going Deeper with Convolutions ）中用来降维，

由于3*3卷积或者5*5卷积在几百个filter的卷积层上做卷积操作时相当耗时，所以1*1卷积在3*3卷积或者5*5卷积计算之前先降低维度。(说的是Channel的维度。)

所以1*1卷积的主要作用有以下几点：
\begin{itemize}
\item 降维

比如，一张500 * 500且厚度depth为100 的图片在20个filter上做1*1的卷积，那么结果的大小为500*500*20。

\item 加入非线性

卷积层之后经过激励层，1*1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力。类似于在一个像素位置，在所有Channel上进行非线性变换，而不考虑附近的信息。

\end{itemize}

所以1*1卷积实际上是对每个像素点，在不同的Channel上进行线性组合， 即信息整合，且保留图像的原有平面结构，调控Depth， 自动完成升维、降维的作用。如下图所示，如果选择2个filter的1*1,那么数据就从原来的depth为3变为depth为2.若用4个filter，则起到了升维的作用。
\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.75\textwidth]{DLTips/OneOneConv0.jpg}
\caption{1*1 Convolutoin 的作用示意图}
\label{OneOneConv0}
\end{figure}

\section{目标检测中完整流程}

由于几个经典的目标检测模型的实现不是很复杂，所以这里主要记录数据的处理过程。首先输入是什么数据、用到了哪些增广技术、输入图像的尺寸、输出是什么形式、目标函数怎么实现怎么优化、怎么评估结果等等。

在目标检测中，主要用到的评估指标是：mAP、IOU。但是怎么实现呢？

目标函数是什么，如何实现BBox的确定等等，好麻烦啊。

\section{Batch Size的影响}

打的批量通常能够让训练收敛更快、性能更强，但会增加内存的开销。

为什么批量更大，能提升性能的两个重要参数：
\begin{itemize}
\item 较大的批量，'可能'会提高优化步骤的有效性，从而导致模型参数更快的收敛
\item 较大的批量也可以减少把训练数据移动到GPU导致的通信开销来提高性能
\end{itemize}

\section{非极大值抑制NMS}

Non-Maximum Suppression, NMS

对于相似的预测边界框，非最大抑制（Non-Maximum Suppression，NMS）只保留置信度最高的那个来去除冗余。它的工作机制如下：\textbf{对于每个类别}，我们首先拿到每个预测边界框被判断包含这个类别物体的概率。然后我们找到概率最大的那个边界框并保留它到输出，接下来移除掉（抑制）其它所有的跟这个边界框的IoU大于某个阈值的边界框。在剩下的属于该类别的边界框里我们再找出\textbf{预测概率最大}的边界框，重复前面的移除过程。到这里我们还只是对其中的一个类别做的事情；完成之后，还需要对其它类别进行同样的事情，也就是去除那些与当前这个类别的重叠较大的边界框。直到我们要么保留或者移除了每个边界框。

\section{Bilinear filler初始化ConvTranspose层的权重}

参考文献：\href{https://www.zhihu.com/search?type=content&q=Transpose%20Convolution%20%E5%88%9D%E5%A7%8B%E5%8C%96}{Bilinear Filler初始化-知乎}

当ConvTranspose层的权重的学习率被初始化为0时，该层的权重在训练过程中保持不变，一直作为Bilineare Resize的作用。

MXNet中，Bilinear Filter Initializer实现代码：

\lstset{language=Python, caption={Bilinear Filter Initializer实现代码-MXNet}}

\begin{lstlisting}
class Bilinear(Initializer):

"""Initialize weight for upsampling layers."""
def __init__(self):
    super(Bilinear, self).__init__()
def _init_weight(self, _, arr):
    weight = np.zeros(np.prod(arr.shape), dtype='float32')
    shape = arr.shape
    f = np.ceil(shape[3] / 2.)
    c = (2 * f - 1 - f % 2) / (2. * f)
    for i in range(np.prod(shape)):
        x = i % shape[3]
        y = (i / shape[3]) % shape[2]
        weight[i] = (1 - abs(x / f - c)) * (1 - abs(y / f - c))
    arr[:] = weight.reshape(shape)
\end{lstlisting}

上述的完成以下公式：

\begin{displaymath}
weight[i] = (1 - abs(x / f - c)) * (1 - abs(y / f - c)) 
\end{displaymath}

虽然具体的公式来源没找到，但可以借鉴下面的Bilinear Interpolation公式：

%\begin{displaymath}
\begin{align*}
f(x, y)  & \approx \frac{y_2 - y}{y_2 - y_1} f(x, y_1) + \frac{y-y_1}{y_2 - y_1}f(x, y_2) \\
 & =  \frac{y_2 - y}{y_2 - y_1} \left( \frac{x_2 - x}{x_2 - x_1} f(Q_{11}) + \frac{x - x_1}{x_2 - x_1} f(Q_{21}) \right) + \frac{y - y_1}{y_2 - y_1} \left( \frac{x_2 - x}{x_2 - x_1} f(Q_{12}) + \frac{x - x_1}{x_2 - x_1} f(Q_{22}) \right) \\
 & = \frac{1}{(x_2 - x_1)(y_2 - y_1)} \left( f(Q_{11})(x_2 - x)(y_2 - y) + f(Q_{21})(x - x_1)(y_2 - y) \right.\\
 & + \left. f(Q_{12})(x_2 - x)(y - y_1) + f(Q_{22})(x - x_1)(y - y_1) \right)
\end{align*}
%\end{displaymath}

参考的知乎答案中，给出了一个插值的例子，可以参考下。

\section{卷积层输出的尺寸}

假设输入尺寸是:$(B, C, W, H)$

且卷积核的大小是$(K, K)$, Stride大小为$(S, S)$, Padding的大小是$Pading$, 那么输出的大小是：
\begin{displaymath}
W_{out} = \frac{W - K + 2 * Padding}{S} + 1
\end{displaymath}

需要注意的是，如果公式中的除法除不尽，那么需要舍去小数部分，也就是，公式中的分式是整除！

\section{机器学习面试博客总结2018.06.11}

\subsection{大疆}

参考文献：\href{https://blog.csdn.net/weixin_37699515/article/details/80023498}{大疆2018机器学习笔记}

\subsubsection{机器学习中的范数问题}

L0范数指向量中非0元素的个数，L1范数是指各个元素的绝对值之和，也叫稀疏规则算法；L2范数就是先平方在求和开方。L2范数有助于处理Condition number不好的情况下矩阵求逆困难的问题，有助于有噪声的情况。

更多的内容看花书部分的总结。

\subsubsection{类别中不平衡}

1. 过抽样
抽样处理不平衡数据的最常用方法，基本思想就是通过改变训练数据的分布来消除或减小数据的不平衡。

过抽样方法通过增加少数类样本来提高少数类的分类性能 ，最简单的办法是简单复制少数类样本，缺点是可能导致过拟合，没有给少数类增加任何新的信息。改进的过抽样方法通过在少数类中加入随机高斯噪声或产生新的合成样本等方法。

2.引入代价敏感因子，设计出代价敏感的分类算法。通常对小样本赋予较高的代价，大样本赋予较小的代价，期望以此来平衡样本之间的数目差异。如 Focal Loss

3.特征选择

样本数量分布很不平衡时，特征的分布同样会不平衡。尤其在文本分类问题中，在大类中经常出现的特征，也许在稀有类中根本不出现。因此，根据不平衡分类问题的特点，选取最具有区分能力的特征，有利于提高稀有类的识别率 。

4. 直接基于原始训练集进行学习，但是在用训练好的分类器进行预测的时候，将上面的第一种方法嵌入到决策过程中，这种方法称为“阈值移动(threshold-moving)”。

5. Hard Negative Mining,

也就是仅利用负类(很多)的少量样本进行训练，也就是改变数据的分布的方式。

\subsubsection{Confusion Matrix}

如图\ref{ConfusionMatrix0}所示。

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.95\textwidth]{DLTips/ConfusionMatrix0.png}
\caption{Confusion Matrix示意图}
\label{ConfusionMatrix0}
\end{figure}

\subsubsection{ROC \& AUC}

ROC曲线，恒周围假阳性概率(FP)， 真阳性(TP)为纵轴组成的坐标图。

真阳性又称为灵敏度。

ROC曲线越靠近左上角，实验的准确性就越高。

AUC为ROC曲线下的面积，哪一种实验的AUC最大，则哪一种实验的诊断判断性最佳。

AUC越接近于1,说明效果越好。

\subsection{机器学习面试总结}

参考文献：\href{https://www.cnblogs.com/huanyi0723/p/8470866.html}{机器学习面试总结-博客园}


\section{花书前两部分总结}

\subsection{第三章}

\subsubsection{概率论的知识}

Sigmoid函数：

\begin{displaymath}
\sigma (x) = \frac{1}{1 + \exp(-x)}
\end{displaymath}

其导数：
\begin{displaymath}
\frac{d}{dx}\delta (x) = \sigma(x) \left( 1 - \sigma (x) \right)
\end{displaymath}

并且有：
\begin{displaymath}
1 - \sigma(x) = \sigma(-x)
\end{displaymath}

\subsubsection{信息论}

一个事件$x$的自信息为：

\begin{displaymath}
I(x) = -\log(Pr(x))
\end{displaymath}

自信息只处理单个输出。我们可以用香农熵来对整个概率分布中的不确定性总量进行量化：

\begin{displaymath}
H(x) = \mathbb{E}_{x\sim P}\left[ \log Pr(x) \right]
\end{displaymath}

对于同一个随机变量x，有两个单独的概率分布$P(x)$和$Q(x)$，\textbf{可以使用KL散度来衡量两个分布的差异}！

\begin{align*}
D_{KL}(P \parallel Q) & = \mathbb{E}_{x \sim P} \left[ \log \frac{P(x)}{Q(x)} \right] \\
					& = \mathbb{E}_{x\sim P} \left[ \log P(x) - \log Q(x) \right]
\end{align*}

从上面的公式可以看出，假设输入的x服从P的分布，计算此时的P与Q之间的差异。

KL散度是非负的，并且衡量的是两个分部之间的差异，它经常被用作分布之间的某种距离。然而他并不是真正的距离，因为它不是对称的。通过选择$D_{KL}(P \parallel Q)$和$D_{KL}(Q \parallel P)$对结果影响较大。

一种与KL散度联系密切的量是交叉熵。即$H(P, Q) = D_{KL}(P \parallel Q) + H(P)$, 可以得到下面的公式：
\begin{displaymath}
H(P, Q) = -\mathbb{E}_{x \sim P} \left[ \log Q(x) \right]
\end{displaymath}

针对分布Q最小化交叉熵等价于最小化KL散度，因为分布Q并不参与被省略的那一项。一种特殊的情况：$\lim_{x\leftarrow 0} x\log x = 0$。

个人补充：

后面我们可以看到，\textbf{这个交叉熵的优化等价于最大似然的优化}！如果但看上面交叉熵的计算公式的话，可以这么认为：分布$P$是数据的真实分布，$Q$是模型的分布，我们优化的目的就是让这两种分布之间的差距最小；然而一般情况下我们并不知道真是的数据分布，一种直观的方法就是用训练数据的经验分布代替真实的的数据分布。

\subsubsection{结构化概率模型}

这部分的有向、无向图模型更详细的内容可以参考<<计算机视觉-模型、学习和推理>>或者<<统计学习方法>>。

\subsection{第四章}

数值计算

\subsubsection{上溢和下溢}

必须对上溢和下溢进行数值稳定的一个例子是\textbf{Softmax函数}，该函数后面可以看到，经常用于预测和Multinoulli分部相关联的概率，定义如下：
\begin{displaymath}
softmax(x)_i = \frac{\exp(x_i)}{\sum_{j=1}^{n}\exp(x_j)}
\end{displaymath}

若所有的$x_j = c$，那么softmax结果为$\frac{1}{n}$。从数值上来说，当c量级很大时，这可能不会发生什么。如果c很小的负数，那么$\exp(c)$就会下溢，这就意味着softmax的分母变为0，所以结果是未定义的；另一方面，如果c很大的正数，那么$\exp(x)$本身就会上溢，同样导致未定义的结果。

解决办法：

构建新的变量：$\mathbf{z} = \mathbf{x} - \max_i x_i$, 然后用z代替x，即：
\begin{displaymath}
softmax(\mathbf{z})
\end{displaymath}

从公式中可以看出，softmax解析上的函数值不会因为从输入向量减去或加上标量而改变。减去$\max_i x_i$ 导致exp的最大参数为0， 这排除了exp函数的上溢的可能性；同样的，分母中至少有一个值为1的项，这就排除了因分母下溢而导致的零除的可能性。

\subsubsection{病态条件}

首先引入条件数的概念，条件数指的是函数相对于输入的微小变化而变化的快慢程度。也就是说，输入微小扰动，输出也可能迅速变化。

考虑函数$f(x)= \mathbf{A}^{-1} \mathbf{x}$, 当$A \in R^{n * n}$具有特征分解时，其条件数为：
\begin{displaymath}
\max_{i, j} \left| \frac{\lambda_i}{\lambda_j} \right|
\end{displaymath}

这里与通常的条件数定义有所不同，即取最大和最小特征值的模之比。当该数很大时，矩阵求逆对输入的误差特别敏感。后面可以看到，L2正则项对病态有一些帮助。

\subsubsection{基于梯度的优化方法}

函数在$\mathbf{u}$方向的方向导数，是函数f在u方向的斜率。换句话说，方向导数是函数$f(x+\alpha u)$关于$\alpha$的导数。使用\textbf{链式法则}，我们可以看到当$\alpha=0$时，有：

\begin{displaymath}
\frac{\delta}{\delta \alpha} f(\mathbf{x} + \alpha \mathbf{u}) = \mathbf{u}^T \nabla_x f(\mathbf{x})
\end{displaymath}

为了最小化f，我们希望找到使f下降的最快的方向。通过上面公式可以看出，当$\mathbf{u}$和$\nabla_x f(\mathbf{x})$的方向相反时，它俩的夹角为0， cos值为1，得到最大。所以这又被称为最速下降法或梯度下降。

梯度下降决定了下降最快的方向，我们还需要设定在该方向的步长，也就是学习率了， 一种选择学习率的方法是线搜索。

\subsubsection{梯度之上}

Jacobian和Hessian矩阵。

Hessian是实对称矩阵，可以分解成$H = Q\varLambda Q^T$。在特定方向$\mathbf{d}$上的二阶导数可以写成$d^THd$。当d是H的一个特征向量时，这个方向的二阶导数就是对应的特征值。对于其他方向的$d$，方向的二阶导数是所有特征值的加权平均。最大特征值确定最大二阶导数，最小特征值确定最小二阶导数。

将函数$f(x)$进行二级泰勒级数近似，则可以计算求得在进行$\epsilon g$步长更新后，其函数值为：
\begin{displaymath}
f(x^{(0)}-\epsilon g) = f(x^{(0)}) - \epsilon g^T g + \frac{1}{2}g^T H g
\end{displaymath}
其中有三项，函数的原始值、函数斜率导致的预期改善、函数曲率导致的矫正。为了使$f(x^{(0)}-\epsilon g) $, 那么上式的右边应该使$-\epsilon g^T g + \frac{1}{2}g^T H g$最小，为了确定最优的学习率，那么把后面这一项看做是$\epsilon$的二阶方程，求解其最小值，应该使得一阶导数为0。对后一项进行对$\epsilon$求导，则得到最优的学习率为：
\begin{displaymath}
\epsilon^{*} = \frac{g^Tg}{g^THg}
\end{displaymath}

在最坏的情况下，g与H的最大特征值$\lambda_{max}$对应的特征向量对齐，此时最优步长为$\frac{1}{\lambda_{max}}$。Hessian 的特征值决定了学习率的量级。

Hessian的另一个作用就是：二阶导数测试。在临界点(一阶导数为0)时，若Hessian是正定的，则该临界点是\textbf{全局最小点}，因为方向二阶导数在任意方向都是正的$d^THd > 0$，所以该临界点的导数导数都是正的，也就是该点是全局最小点了。当Hessian矩阵是负定的时候，该点是\textbf{局部极大点}。如果Hessian的特征值有大于0的值，也有小于0的值，则为鞍点。

当Hessian的条件数很差时，梯度下降法也会表现的很差。这是因为一个方向上的导数增加的很快，而在另一个方向上增加的很慢。梯度下降不知道导数的这种变化，所以它不知道应该优先搜索导数长期为负的方向，而不是曲率最大的方向。病态条件也导致很难选择合适的补偿。

我们可以使用Hessian矩阵的信息来指导搜索。比如牛顿法。

\subsubsection{约束优化}

包含著名的KKT条件+广义拉格朗日了。


\subsection{第五章}

机器学习基础

\subsubsection{设计矩阵}

设计矩阵的每一行包含一个不同的样本，每一列对应不同的特征。

\subsubsection{容量、过拟合和欠拟合}

模型的容量：通俗来讲，模型的容量是指其拟合各种函数的能力。容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质，所以从这个角度来看，增加正则项来防止过拟合本质上是限制模型可以拟合的函数的空间的大小。

VC维度：定义为该分类器能够分类的训练样本的最大数目。

正则化：是指修改学习算法、使其降低泛化误差而非训练误差。

正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相提并论。这一点从花书的第七章、第八章可以看出来，第七章全面的讲了正则化、第八章全面的讲了优化算法。

\subsubsection{估计、偏差和方差}

偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或参数的误差期望，而方差度量着数据上任意特定采样可能导致的估计期望的偏差。

均方误差(MSE):

\begin{align*}
MSE & = \mathbb{E} \left[ (\hat{\theta}_m - \theta)^2 \right] \\
   & = Bias(\hat{\theta}_m)^2 + Var(\hat{\theta}_m)
\end{align*}

MSE度量着估计与真实参数$\theta$之间的平方误差的总体期望偏差。如上式所示，MSE包含了偏差和方差。理想的估计具有较小的MSE或是在检查中会稍微约束他们的偏差和方差。

偏差和方差的关系与机器学习容量、欠拟合和过拟合的概念紧密相连。用MSE度量泛化误差(偏差和方差对于泛化误差都是有意义的)时，增加容量会增加方差，降低偏差。

补充一点：

\href{https://www.zhihu.com/question/20448464/answer/20039077}{偏差和方差的区别}

偏差：描述的是预测值的期望与真实值之间的差距。

方差：描述的是预测值的变化范围，离散程度，也就是距离其期望值的距离。

当偏差较大时，一般发生欠拟合；当方差较大时，泛化能力较差，一般发生过拟合。随着模型容量的增加，偏差下降，方差呈U型。

训练程度与偏差、方差的关系：

1. 训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器发生显著变化，偏差将主导泛化错误率。

2. 当训练程度加深，学习器的能力逐渐增强，训练数据发生的扰动逐渐能够被学习器学到，方差将主导泛化错误率。

3. 训练程度充足后，学习器的拟合能力已经非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化。将发生过拟合。

\subsubsection{最大似然估计}

这一部分主要推导下前面提到的，最大似然估计与交叉熵之间的关系。

最大似然估计的目标是：

\begin{align*}
\mathbf{\theta}_{ML} &= \argmax_{\mathbf{\theta}} p_{model} (\mathcal{X}; \mathbf{\theta}) \\
					& = \argmax_{\mathbf{\theta}} \prod_{i=1}^{m} p_{model}(x^{(i)}; \mathbf{\theta})
\end{align*}

其中，m是样本数量。

推导如下，将上面的最大似然转换成对数形式,则乘积编程加法:

\begin{align*}
\mathbf{\theta}_{ML} & = \argmax_{\mathbf{\theta}} \sum_{i=1}^{m} \log p_{model}(x^{(i)}; \mathbf{\theta}) \\
					& = \argmax_{\mathbf{\theta}} \mathbb{E}_{x \sim \hat{p}_{data}} \log p_{model}(x; \mathbf{\theta})
\end{align*}

上面的推导成立的原因：因为当重新缩放代价函数时，argmax的结果不会改变，我们可以除以m得到和训练数据经验分布$\hat{p}_{data}$相关的期望作为准则，为啥会是$\hat{p}_{data}$而不是$p_{model}$呢，因为输入的数据$x$是来自训练数据而不是模型。

一种，解释最大似然估计的观点是将它看做最小化训练集上的经验分布和模型分布之间的差异，两者之间的差异程度可以通过KL散度度量：

\begin{align*}
D_{KL}\left( \hat{p}_{data} \parallel p_{model} \right) = \mathbb{E}_{x\sim \hat{p}_{data}} \left[ \log \hat{p}_{data}(x) - \log p_{model} (x) \right]
\end{align*}

在上式中，由于左边一项仅设计数据的生成过程，和模型无关。这意味着最小化KL散度时，可以省略第一项，得到：

\begin{displaymath}
-\mathbb{E}_{x \sim \hat{p}_{data}} \left[ \log p_{model}(x) \right]
\end{displaymath}

对比上式与前面的最大似然的式子，可以发现，最小化KL散度就是最小化分布之间的交叉熵，也就等价于最小化负对数似然，也就是最大似然了。

因为最大似然具有：一致性、统计效率，使其成为机器学习中的首选估计方法。

\subsubsection{贝叶斯估计}

这里主要说明最大后验(MAP)估计与后面正则项的关系！

MAP的公式为：

\begin{align*}
\mathbf{\theta}_{MAP} & = \argmax_{\mathbf{\theta}} p(\mathbf{\theta} | \mathbf{x}) \\
				& = \argmax_{\mathbf{\theta}} \left[ \log p(\mathbf{x}|\mathbf{\theta}) + \log p(\mathbf{\theta}) \right]
\end{align*}

公式默认省略了数据的生成分布$p(\mathbf{x})$。从上面可以看出来，相比于ML，MAP多了一项：$ p(\mathbf{\theta})$， 这一项就是参数$\mathbf{\theta}$的先验概率分布了，该先验信息有助于减少最大后验点估计的方差，但缺点是增加了偏差。更重要的，看上面的公式：是不是跟带有正则项的目标函数比较相似。事实上，许多正则化估计方法，例如权重衰减正则化的最大似然学习，可以解释为贝叶斯推断的MAP近似。这个是英语正则化时加到目标函数的附加项对应着$\log p(\theta)$。但并非所有的正则化惩罚对应着MAP贝叶斯推断。

MAP贝叶斯推断提供了一个直观的方法来设计复杂但可解释的正则化。

\subsubsection{随机梯度下降}

随机梯度下降的核心是，梯度是期望(这一点可以从训练数据集上的梯度计算可以看出来，是一个求平均的过程)。既然是期望，就可以用小规模的样本近似估计。一个非常重要的优势：使用随机梯度下降，当总的样本数m不断增加时，我们都可以只使用小批量$m^{'}$来计算梯度估计，而且该模型最终会在随机梯度下降抽样完训练集上的所有样本之前收敛到可能的最优测试误差。继续增加m不会延长达到模型可能的最优测试误差的时间，从这个角度来看，SGD训练模型的渐进代价是关于m的函数的$O(1)$级别的！！！

\subsubsection{促使深度学习发展的挑战}

\begin{itemize}
\item 维数灾难
\item 局部不变性和平滑正则项
\item 流行学习
\end{itemize}

细节就不说了。

\subsection{第六章}

深度前馈网络

\subsubsection{基于梯度的学习}

代价函数：

1. 使用最大似然学习条件分布，正如前面说过的，基于最大似然的优化等价于基于KL散度和交叉熵的优化，准确的说，与训练数据和模型分布间的交叉熵等价。等价函数表示为：
\begin{displaymath}
J(\mathbf{\theta}) = - \mathbb{E}_{\mathbf{x}, y \sim \hat{y}_{data}} \log p_{model}(y | \mathbf{x})
\end{displaymath}

代价函数的具体形式与模型相关，取决于$\log p_{model}$的形式，比如当$p_{model}$具有高斯分布的形式时，最大后验概率具有均方误差的形式。具体的推导只需要把高斯分布的公式带入到上式就行。

使用最大似然来导出代价函数的方法的一个优势是，它减轻了为每个模型设计代价函数的负担。

贯穿神经网络设计的一个反复出现的主题就是代价函数的梯度必须足够大和具有足够的预测性。很多输出单元都会包含一个指数函数，这在它的变量取绝对值非常大的赋值时会造成饱和，所以，\textbf{通过负对数的形式可以在一定程度上抵消输出层的指数效果}。

2. 学习条件概率统计量

这是另一种代价函数，可以吧代价函数看做一个\textbf{泛函}。而不仅仅是一个函数，泛函的意思是函数到实数的映射。常用到变分法，这个方法可以在花书第19章找到。

输出单元：

任何可用作输出的神经网络单元也可以用于隐藏单元。通过下面的问题，可以看出，输出单元的形式(输出函数)与模型的分布形式相关。

1. 用于高斯输出分布的线性单元

给定特征h， 线性输出单元产生一个分量，$\hat{y}=\mathbf{W}^T h + b$， 由于输出还具有高斯函数的形式，因此等价函数就是前文提到的等价于均方误差了。

多说一句，为啥要用线性单元呢？因为对于高斯输出分布，我们只需要知道两个连续的变量就可以，一个是期望，一个是方差；这两个值基本上没什么限制，最大的特点就是它们是连续的值，所以用线性单元就可以了。

2. 用于Bernoulli输出分布的sigmoid单元

为啥要引入sigmoid单元呢，因为该函数的可以将所有的输入压缩到$(0, 1)$区间。对于Bernoulli输出，我们需要输出的是一个用于二分类的概率的值，但概率需要满足在0到1之间，因此Sigmoid就刚好合适。Sigmoid输出单元的形式如下：

\begin{displaymath}
\hat{y} = \sigma(z), z = \mathbf{\omega}^T\mathbf{h} + b
\end{displaymath}

下面简单推导一下输出单元是sigmoid时的代价函数形式，还是基于最大似然实现代价函数。需要强调一下，若要使用最大似然，我们需要知道$p_{model}(y | x)$的形式，在Bernoulli分布中，首先假定费归一化的对数概率对y和z是线性的(不明白为什么可以)，并且先不考虑x的条件，那么$\hat{y}_{model}(y)$为：

\begin{align*}
\log \tilde{P}(y) & = yz\\
\tilde{P}(y) & = \exp(yz)\\
P(y) &= \frac{\exp(yz)}{\sum_{y'=0}^{1}\exp(y'z)}\\
P(y) & = \sigma((2y - 1)z)
\end{align*}

需要说明的一点：基于指数和归一化的概率分布在统计建模的文献中很常见。用于定义这种二值型变量分布的变量z被称为分对数。

所以损失函数就是：

\begin{align*}
J(\mathbf{\theta}) &= -\log P(y|x) \\
	& = -\log \sigma((2y - 1)z) \\
	& = \zeta ((1-2y)z)
\end{align*}

其中，$\zeta(x) = \log (1 + \exp(x))$是Softplus函数。

几点重要的性质：

通过分析上面的损失函数，可以看到，只有在$(1-2y)z$取绝对值很大的负值时才会饱和。因此饱和只会出现在模型应得到正确答案的时候。反正对于极限情况下极度不正确的z， softplus函数完全不会收缩梯度。这个性质可以让基于梯度的学习可以很快的改正错误。

最大似然几乎总是训练sigmoid输出单元的优选方法。

3. 用于Multinoulli输出分布的softmax单元

主要看一下基于softmax输出单元的代价函数长什么样。

softmax函数：
\begin{displaymath}
softmax(z)_i = \frac{\exp(z)_i}{\sum_{j}\exp(z_j)}
\end{displaymath}

则负对数似然的形式是：
\begin{displaymath}
\log softmax(z)_i = z_i - \log \sum_{j} \exp(z_j)
\end{displaymath}

上式中，第一项表示输入$z_i$总是对代价函数有直接的贡献。因为这一项不会饱和，因此及时$z_i$对第二项的贡献很小，学习依然可以进行！当最大化对数似然时，第一项会一直鼓励$z_i$被推高，而第二项则鼓励所有的z被压低。注意，可以把第二项大致近似为$\max_j z_j$。这种近似对任何明显小于 $\max_j z_j$的$z_k, \exp(z_k)$都是不重要的。所以，可以认为负对数似然总是强烈的惩罚最活跃的不正确的预测。也就是说，如果当前的$z_i$就是所有的z中的最大值，那么这两项大致抵消，也就是没有多少损失；但如果不是最大的值，那么就会产生一个差值，而且对于负对数形式(上面公式乘以-1)而言，第一项负数的绝对值小于第二项的正数，因此会贡献一个正的损失。

额外的一点是，softmax正如前文提到的，可能会遇到饱和的情况，这种背景下的解决办法就是前文提到的方法了。

最后说一点，softmax跟神经元之间的侧抑制类似，极端情况下会出现赢者通吃的形式。

4. 其它输出类型

\subsubsection{隐藏单元}

1. 整流线性单元及其扩展

\begin{itemize}
\item ReLU

\begin{displaymath}
ReLU(x) = \max(0, x)
\end{displaymath}

优点是：已与优化， 因为他们和线性单元非常相似。由于，在激活状态时，一阶导数处处为1,这意味着相比于引入二阶效应的激活函数来说，它的梯度方向对于学习来说更加有用。

缺陷是：它们不能通过基于梯度的学习方法学习那些使它们激活为0的样本。也就是在非激活状态下，梯度为0了。

针对上面的缺陷，主要的改进下式中的$\alpha$的不同得到集中不同的改进方法：

\begin{displaymath}
max(0, x_i) + \alpha_i \min(0, x_i)
\end{displaymath}

\item 绝对值整流

此时：$\alpha_i = -1$

\item 渗漏整流线性单元 Leaky ReLU

此时，$\alpha_i$被固定成一个类似0.01的小值。

\item 参数化整流线性单元 PReLU

此时, 将$\alpha_i$是可以学习的参数。

\item maxout

进一步扩展了整流线性单元，就是将z划分成具有k个值的组，即每一组具有k个值。每个maxout单元则输出每组中的最大元素。

maxout可以学习具有多达k段的分段线性的凸函数。maxout单元因此可以视为学习激活函数本身，而不仅仅是单元之间的关系。使用足够大的k，maxout可以以任意精度近似任何凸函数。maxout具有一些冗余来帮助抵抗灾难遗忘的现象，即神经网络忘记了如何执行它们过去训练的任务。

\end{itemize}

整流线性单元及其扩展都是基于一个原则，就是如果它们的行为更接近线性，那么模型更容易优化。

2. Logistic sigmoid

\begin{displaymath}
g(z) = \sigma(z)
\end{displaymath}

sigmoid函数在大多定义域内都是饱和的，仅在0附近梯度比较大。这使得基于梯度的学习变得非常困难。现在不鼓励将其作为隐藏单元了。但在循环网络、许多概率模型以及一些自编码器中还是比较有吸引力的。

3. 双曲正切函数

\begin{displaymath}
g(z) = \tanh(z) = 2\sigma(2z) - 1
\end{displaymath}

如果必须使用sigmoid激活函数时，双曲正切激活函数更好。并且tanh(0) = 0, 而$\sigma(0) = 1/2$， 使后者看上去更像单位函数。因为tanh函数在0附近跟单位函数更向。

4. 其它

包括：径向基函数，此函数大部分输入容易饱和，很难优化；softpluse，是整流线性单元的平滑版本，不鼓励使用；硬双曲正切函数。

5. 个人补充

比较好的总结：\href{https://zhuanlan.zhihu.com/p/21568660}{激活函数面面观-知乎}

激活函数需要的一些性质：
\begin{itemize}
\item 非线性

当激活函数是非线性(按照后面的万能近似性质，这里原文可能有误，所以改成非线性)的时候，一个两层的神经网络就可以逼近基本上所有的函数了。

\item 可微性

用于基于梯度的学习。

\item 单调性

当激活函数是单调的时候，单层网络能够保证是凸函数。

\item 输出值的范围

当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.(不太理解！)

\end{itemize}

对于每一个激活函数，简单补充如下：

\begin{itemize}
\item Sigmoid

它能够把输入的连续实值“压缩”到0和1之间。

缺点：
\begin{itemize}
\item Sigmoids saturate and kill gradients. sigmoid 有一个非常致命的缺点，当输入非常大或者非常小的时候，这些神经元的梯度是接近于0的，从图中可以看出梯度的趋势。所以，你需要尤其注意参数的初始值来尽量避免saturation的情况。如果你的初始值很大的话，大部分神经元可能都会处在saturation的状态而把gradient kill掉，这会导致网络变的很难学习。

\item Sigmoid 的 output 不是0均值. 这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。产生的一个结果就是：如果数据进入神经元的时候是正的$(e.g. x>0 elementwise in f=w^T x + b)$，那么针对 w 计算出的梯度(简单的可以认为就是$x$， 见本章的后向传播的解释)也会始终都是正的。


\item 当然了，如果你是按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的 kill gradients 问题相比还是要好很多的。
\end{itemize}

\item tanh

与 sigmoid 不同的是，tanh 是0均值的。因此，实际应用中，tanh 会比 sigmoid 更好（毕竟去粗取精了嘛）。

\item ReLU

梯度消失 和 梯度爆炸 在relu下都存在， sigmoid下没有梯度爆炸。

随着 网络层数变深， activations倾向于越大和越小的方向前进， 往大走梯度爆炸（回想一下你在求梯度时， 每反向传播一层， 都要乘以这一层的activations）， 往小走进入死区， 梯度消失。

这两个问题最大的影响是， 深层网络难于converge。

BN和xavier初始化（经指正， 这里最好应该用msra初始化， 这是he kaiming大神他们对xavier的修正， 其实就是xavier多除以2）很大程度上解决了该问题。

\href{https://www.zhihu.com/question/66027838}{怎么理解梯度弥散和梯度爆炸？-知乎}

个人认为梯度不稳定的根本原因在于神经网络的高度非线性，这使得优化过程中既存在很小的梯度，同时也存在很大的梯度

relu只是缓解了梯度消失的问题，并不是解决了梯度消失问题，前向神经网络由于每层的权重相关性较小，bp过程相乘之后相对来说不易产生消失的梯度，再加上relu本身不具饱和性，所以才会取得很好的效果。但是并不是所有的神经网络都用relu，比如循环神经网络，当然也可以通过加入一些线性通路来缓解。

ReLU的负半轴梯度为0，所以有时候（比较少见）也还是会梯度消失，这时可以使用PReLU替代，如果用了PReLU还会梯度弥散和爆炸，请调整初始化参数，对自己调参没信心或者就是懒的，请直接上BN。

至于sigmoid为什么会有梯度消失现象，是因为sigmoid(x)在不同尺度的x下的梯度变化太大了，而且一旦x的尺度变大，梯度消失得特别快，网络得不到更新，就再也拉不回来了。我想sigmoid函数的情况，可以通过sigmoid函数的求导看出来了，因为求导后的结果同样包含sigmoid函数！

\end{itemize}

\begin{figure}[!hbtp]
\centering
\includegraphics[width=0.5\textwidth]{DLTips/ActivationFunctions0.jpg}
\caption{激活函数对比}
\label{ActivationFunctions0}
\end{figure}

梯度弥散就是梯度消失。

\subsubsection{架构设计}

主要几点：

\begin{itemize}
\item 万能近似性质

万能近似定理：一个前馈神经网络如果具有线性输出层和至少一层具有任何一种挤压性质的激活函数，例如sigmoid，的隐藏层， 只要给予足够数量的隐藏单元，它可以以任意的精度来近似任何一个从有限维空间到另一个空间有限维空间的Borel可测函数。

\item 深度

Montufar等人指出，一些用深度深度整流网络表示的函数可能需要浅层网络(一个隐藏层)指数级的隐藏单元才能表示！一个公式：具有d个输入、深度为l、每个隐藏层具有n个单元的深度整流网络可以描述的线性区域的数量是：

%\begin{displaymath}
%O\left( \begin{array}[cc]
%
%\end{array} \right)
%\end{displaymath}

意味着，这是深度l的指数级。

\item 其它架构上的考虑

比如跳跃连接等。

\end{itemize}

\subsubsection{反向传播}

这部分单独写。感觉重要的一个公式是：微积分中的链式法则。

\subsection{第七章}

深度学习中的正则化

根据第五章的定义，正则化是对学习算法的修改----旨在减少泛化误差而不是训练误差。

目前有许多正则化策略，有些策略向机器学习模型添加限制参数值的额外约束、有些策略向目标函数添加额外项对参数值进行软约束。

估计的正则项以偏差的增加换取方差的减少。

按照花书的意思，正则化的目标是使模型从过拟合状态变为正常状态。

\subsubsection{参数范数惩罚}

其实，主要是L1、L2范数正则项。

\begin{itemize}
\item L2参数范数正则化

通常被称为权重衰减(weight decay), 又叫岭回归或Tikhonoy正则。

前面提到了，L2范数有助于病态条件下的优化。这是因为加入权重衰减的效果是沿着Hessian矩阵H的特征向量所定义的轴缩放未加约束时的最优值$\omega^{*}$。具体来说，我们会根据$\frac{\lambda_i}{\lambda_i+\alpha}$因子缩放与H的第i个特征向量对齐的$\omega^{*}$分量。若该方向特征值较大，则正则化的影响较小，若该方向的特征值较小，则该分量将会收缩到几乎为0.

\item L1参数范数正则化

最终的特性是，会产生更稀疏的解。会起到特征选择的作用。

补充：

\href{https://zhuanlan.zhihu.com/p/35356992}{L1正则化与L2正则化}

再这篇文章中，作者提到了本笔记前面提到的MAP中的重点说明。也就是说，L1范数等价于先验让权重$\omega$服从标准拉普拉斯分布，即概率密度函数为$1/2 \exp(-|x|)$，则MAP多出来的一项就是L1的形式了。而通过拉普拉斯分布可知，当$\omega$取到0的概率特别大。也就是说我们提前先假设了$\omega$的解更容易取到0。

而对于L2范数，其等价于先验让权重$\omega$服从标准正态分布，即概率密度为$1/\sqrt{2\pi} \exp(-(x)^2/2)$，带入这个权重的先验分布，就可以得到L2范数的形式了。根据正态分布可以知道，我们预先假设了$\omega$的最终值可能取到0附近的概率特别大。

结构风险最小化： 在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。

\end{itemize}

\subsubsection{数据集增强}

\subsubsection{噪声鲁棒性}

对于某些模型，向输入添加方差极小的噪声等价于对权重施加范数惩罚。一般情况下，注入噪声远比简单的收缩参数强大，特别是噪声被添加到隐藏单元时，会更加强大，比如Dropout就是这种做法的主要发展方向。

另一种正则化模型的噪声使用方式是将其添加到权重上。这项技术主要用于循环神经网络。

\subsubsection{半监督学习}

\subsubsection{多任务学习}

当模型的一部分被多个额外的任务共享时，这部分将被约束为良好的值。

\subsubsection{提前终止}

通过分析，可以发现，提前终止与权重衰减有非常大的联系，尤其是权重衰减系数的倒数。

\subsubsection{Bagging等集成方法}

模型平均奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差。

Bagging是一种允许重复多次使用同一种模型，训练算法和目标函数的方法。

\subsubsection{Dropout}

Dropout提供了一种廉价的Bagging集成近似，能够训练和评估指数级数量的神经网络。

Dropout训练与Bagging训练不太一样，在Bagging的情况下，所有模型都是独立的。在Dropout的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。

下面一段话来自Gluon教程：对一个随机采样的一个批量的训练数据分别训练一个元神经网络子集的分类器，且每个神经网络子集使用同一套参数。

\subsubsection{对抗训练}



\subsection{第八章}

深度模型中的优化

\subsubsection{神经网络优化中的挑战}

1. 病态

病态体现在随机梯度下降会卡在某些情况，此时即使很小的更新步长也会增加代价函数。数学原理如下：

将跟新后的代价函数的新值进行泰勒二阶近似：

\begin{displaymath}
f(x_0 - \epsilon g) = f(x_0) - \epsilon g^T g + \frac{1}{2} \epsilon^2 g^T H g
\end{displaymath}

我们主要关注上式的后两项。
\begin{displaymath}
- \epsilon g^T g + \frac{1}{2} \epsilon^2 g^T H g
\end{displaymath}

很明显，当$\frac{1}{2} \epsilon^2 g^T H g$超过$- \epsilon g^T g$时，梯度的病态就会成为问题。虽然牛顿法在解决图优化问题中的病态条件比较有效果，但对神经网络效果不太好。

2. 局部极小值

3. 高原、鞍点和其它平坦区域

4. 悬崖和梯度爆炸

5. 长期依赖

当计算图变得极深时，神经网络优化算法面临的另一个难题就是长期依赖问题。深层的计算图不仅存在于前馈网络中，还存在于循环网络中。其数学原理如下：

假设某个计算图中包含一条反复与矩阵$W$相乘的路径，那么t步后，相当于乘以$W^t$。假设$W$有特征分解$W = V diag(\lambda) V^{-1}$。在这种简单的情况下，很容易看出：
\begin{displaymath}
W^t = (V diag(\lambda) V^{-1})^t = v diag(\lambda)^t V^{-1}
\end{displaymath}

所以，当特征值$\lambda_i$不在1附近时，若在量级大于1则会爆炸；若小于1时则会消失。梯度消失和爆炸问题，是指该计算图上的梯度也会因为$diag(\lambda)$大幅变化。

\subsubsection{基本算法}

\begin{itemize}
\item SGD

\begin{algorithm}
\caption{SGD}
\begin{algorithmic}[1]
\Require 学习率$\epsilon$, 初始参数$\theta$
\Ensure 更新后的参数$\theta$
\Repeat

	从训练样本中随机采样m个样本，构成一个小批量
	
	计算小批量的梯度：
	\begin{displaymath}
	g \leftarrow \frac{1}{m} \sum_{i}\nabla_{\theta}L(f(x^{i}; \theta), y^{i})
	\end{displaymath}
	
	更新参数：$\theta \leftarrow \theta - \epsilon g$
\Until{条件满足}
\end{algorithmic}
\end{algorithm}

虽然随机梯度下降仍然是非常受欢迎的优化方法，但其学习过程有时会很慢。特别是高区率，小但一致的梯度，或是带噪声的梯度。

\item 动量

动量法旨在加速学习过程，特别是处理高曲率，小但一致的梯度，或是带噪声的梯度。动量算法积累之前梯度指数级衰减的移动平均，并且继续沿该方向移动。算法如下：
\begin{algorithm}
\caption{使用动量的SGD}
\begin{algorithmic}
\Require 学习率$\epsilon$, 初始参数$\theta$, 动量参数$\alpha$, 初始速度$v$
\Ensure 更新后的参数
\Repeat
	随机抽样m个样本构成一个小批量
	
	计算小批量上的梯度：
	\begin{displaymath}
		g \leftarrow \frac{1}{m} \sum_{i}\nabla_{\theta}L(f(x^{i}; \theta), y^{i})
	\end{displaymath}
		
	计算动量：$v \leftarrow \alpha v - \epsilon g$
	
	更新： $\theta \leftarrow \theta + v$	
	
\Until{满足停止条件}
\end{algorithmic}
\end{algorithm}

\item Nesterov动量

Nesterov动量与标准动量之间的区别体现在梯度计算上，Nesterov动量中，梯度计算在施加当前速度之后。
\begin{algorithm}
\caption{使用Nesterov动量的SGD}
\begin{algorithmic}
\Require 学习率$\epsilon$, 初始参数$\theta$, 动量参数$\alpha$, 初始速度$v$
\Ensure 更新后的参数
\Repeat
	随机抽样m个样本构成一个小批量
	
	首先更新一次参数：$\tilde{\theta} \leftarrow \theta + v$
	
	
	计算小批量上的梯度：
	\begin{displaymath}
		g \leftarrow \frac{1}{m} \sum_{i}\nabla_{\tilde{\theta}}L(f(x^{i}; \theta), y^{i})
	\end{displaymath}
		
	计算动量：$v \leftarrow \alpha v - \epsilon g$
	
	更新： $\theta \leftarrow \theta + v$	
	
\Until{满足停止条件}
\end{algorithmic}
\end{algorithm}

在凸批量梯度的情况下，Nesterov动量将额外误差收敛率从$O(1/k)$改进到$O(1/k^2)$， 其中k是迭代次数。但在随机梯度的情况下，Nesterov动量没有改进收敛率。

\end{itemize}

\subsubsection{参数初始化策略}

参数的初始点对算法的性能影响很大。也许完全确知的唯一特性是初始参数需要在不同单元间''破坏对称性''。这或许有助于确保没有输入模式丢失在前向传播的零空间中，没有梯度模式丢失在反向传播的零空间中。

现在存在Normalized initialization或者Xaive初始化方法。在该模型下，这个初始化方案确保了达到收敛所需的训练迭代总次数独立于深度。

\subsubsection{自适应学习率算法}

\begin{itemize}
\item AdaGrad

AdaGrad算法，独立地适应所有模型参数的学习率，缩放每个参数反比其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上具有较小的下降，净效果就是在参数空间中，更为平缓的倾斜方向会取得更大的进步。

但有一个缺点：从训练开始时积累平方会导致有效学习率过早或过量的减少。在一些深度学习模型上效果不错，但不是全部。算法如下：
\begin{algorithm}
\caption{AdaGrad算法}
\begin{algorithmic}
\Require 全局学习率$\epsilon$, 初始化参数$\theta$, 小常数$\delta$ , 为了数值稳定，一般设为$10^{-7}$
\Ensure 自适应学习率
\State 初始梯度累计变量$r=0$
\Repeat
	从训练集中随机抽样m个样本构成一个小批量
	
	计算小批量上的梯度：
	\begin{displaymath}
		g \leftarrow \frac{1}{m} \sum_{i}\nabla_{\tilde{\theta}}L(f(x^{i}; \theta), y^{i})
	\end{displaymath}
	
	计算梯度累计量(梯度历史平方值总和)：$r \leftarrow r + g \odot g$
	
	计算更新：
	\begin{displaymath}
	\Delta \theta \leftarrow - \frac{\epsilon}{\sqrt{r}+\delta} \odot g
	\end{displaymath}
	
	应用更新：$\theta \leftarrow \theta + \Delta \theta $
\Until{达到更新准则}
\end{algorithmic}
\end{algorithm}

\item RMSProp

RMSProp算法修改AdaGrad以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均。AdaGrad旨在应用于凸问题时快速收敛。当应用于非凸函数训练时神经网络时，学习轨迹可能穿过很多不同的结构，最终到达一个局部是凸碗的区域。AdaGrad根据平方梯度的整个历史收敛学习率，可能使得学习率在达到这样的凸结构之前就变得太小了。RMSProp使用指数衰减平均以丢弃遥远过去的历史，使其能够再找到凸碗状结构后快速收敛，它像一个初始化于该碗状结构的AdaGrad算法实例。算法如下：
\begin{algorithm}
\caption{RMSProp算法}
\begin{algorithmic}
\Require 全局学习率$\epsilon$, 初始化参数$\theta$, 小常数$\delta$ , 为了数值稳定，一般设为$10^{-7}$, 衰减速度$\rho$
\Ensure 自适应学习率
\State 初始梯度累计变量$r=0$
\Repeat
	从训练集中随机抽样m个样本构成一个小批量
	
	计算小批量上的梯度：
	\begin{displaymath}
		g \leftarrow \frac{1}{m} \sum_{i}\nabla_{\tilde{\theta}}L(f(x^{i}; \theta), y^{i})
	\end{displaymath}
	
	计算梯度累计量(指数加权的移动平均)：$r \leftarrow \rho r + (1 - \rho) g \odot g$
	
	计算更新：
	\begin{displaymath}
	\Delta \theta \leftarrow - \frac{\epsilon}{\sqrt{r}+\delta} \odot g
	\end{displaymath}
	
	应用更新：$\theta \leftarrow \theta + \Delta \theta $
\Until{达到更新准则}
\end{algorithmic}
\end{algorithm}

具有Nesterov动量的RMSProp， 思想大体一致，先暂时更新参数$\theta$，然后用这个更新后的参数计算梯度，就如Nesterov动量SGD一样。

RMSProp是现阶段深度学习从业者经常采用的优化方法。

\item Adam

Adam包含了偏置修正，修正从原点初始化的一阶矩和二阶矩的估计。Adam通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要跟从建议的默认修改。

\begin{algorithm}
\caption{Adam算法}
\begin{algorithmic}
\Require 全局学习率$\epsilon$， 建议为0.001, 初始化参数$\theta$, 小常数$\delta$ , 为了数值稳定，一般设为$10^{-7}$
\Require 矩估计的指数衰减速率$\rho_1$和$\rho_2$, 在区间$[0, 1]$， 建议分别为0.9和0.999
\Ensure 自适应学习率
\State 初始梯度累计变量$s=0$和$r=0$
\Repeat
	从训练集中随机抽样m个样本构成一个小批量
	
	计算小批量上的梯度：
	\begin{displaymath}
		g \leftarrow \frac{1}{m} \sum_{i}\nabla_{\tilde{\theta}}L(f(x^{i}; \theta), y^{i})
	\end{displaymath}
	
	$t \leftarrow t+1$
	
	更新有偏一阶矩估计：$s \leftarrow \rho_1 s + (1 - \rho_1) \odot g$
	
	更新有偏二阶矩估计：$r \leftarrow \rho_2 r + (1 - \rho_2) g \odot g$
	
	修正一阶矩的偏差：$\hat{s} \leftarrow \frac{s}{1 - \rho_1^t}$
	
	修正二阶矩的偏差：$\hat{r} \leftarrow \frac{r}{1 - \rho_2^t}$
	
	计算更新：
	\begin{displaymath}
	\Delta \theta \leftarrow - \epsilon \frac{\hat{s}}{\sqrt{\hat{r}}+\delta} \odot g
	\end{displaymath}
	
	应用更新：$\theta \leftarrow \theta + \Delta \theta $
\Until{达到更新准则}
\end{algorithmic}
\end{algorithm}

\item AdaDelta

RMSProp针对Adagrad在迭代后期可能较难找到有用解的问题，对小批量随机梯度按元素平方项做指数加权移动平均而不是累加。另一种应对该问题的优化算法叫做Adadelta。有意思的是，它没有学习率超参数。

Adadelta算法也像RMSProp一样，使用了小批量随机梯度按元素平方的指数加权移动平均变量s，并将其中每个元素初始化为0。  给定超参数$\rho$且$0≤\rho<1$， 在每次迭代中，RMSProp首先计算小批量随机梯度g，然后对该梯度按元素平方项$g\odot g$做指数加权移动平均，记为s：

\begin{displaymath}
s \leftarrow \rho s + ( 1- \rho) g \odot g
\end{displaymath}

然后，计算当前需要迭代的目标函数自变量的变化量$g'$：

\begin{displaymath}
g' \leftarrow \frac{\sqrt{\Delta x + \delta}}{\sqrt{s + \delta}} \odot g
\end{displaymath}

其中$\delta$为数值稳定而添加的常数。和Adagrad与RMSProp一样，目标函数自变量中每个元素都分别拥有自己的学习率。上式中的$\Delta x$初始化为零张量，并记录$g'$暗元素平方的指数加权移动平均：
\begin{displaymath}
\Delta x \leftarrow \Delta x + (1 - \rho)g' \odot g'
\end{displaymath}

最后，自变量迭代步骤和小批量随机梯度下降类似：
\begin{displaymath}
x \leftarrow x - g'
\end{displaymath}

\end{itemize}

一些资料：\href{https://zhuanlan.zhihu.com/p/33385885}{Adam优化算法-知乎}

\subsubsection{选择正确的优化算法}

花书说，目前最流行的并且使用很高的优化算法包括SGD, 包含动量的SGD, RMSProp, 具有栋梁的RMSProp， AdaDelta和Adam。选择哪一个算法似乎主要取决于使用者对算法的熟悉程度，以便调节超参数。

\subsubsection{优化策略和元算法}

1. 批标准化

2. 坐标下降

3. 监督与训练

等


\section{梯度消失-梯度爆炸 专题}

参考文献[1]: \href{https://zhuanlan.zhihu.com/p/25631496}{神经网络训练中的梯度消失和梯度爆炸-知乎}

本质上是因为梯度反向传播中的连乘效应。

参考文献[2]: \href{https://www.zhihu.com/question/49812013}{BP算法中为什么产生梯度消失-知乎}



\section{待续}













